{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEMANT IS MY NAME',\n",
       " 'HEEEMMMANNNNT IIIIS MMMMY N NAAAME EE',\n",
       " 'DRNOT TL KTCRNT ST RCT S C CDR C T DGT NE']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [\n",
    "    \"Hemant is my name\",\n",
    "    \"Heeemmmannnnt iiiis mmmmy n naaame ee\",\n",
    "    \"DRNOT TL KTCRNT ST RCT S C CDR C T DGT NE\"\n",
    "]\n",
    "\n",
    "input_texts = [\"\".join(i.upper()) for i in input_texts ]\n",
    "input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/rajivratn/anaconda3/envs/langspeech/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/home/rajivratn/anaconda3/envs/langspeech/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(375, 1536, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-28): 29 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (k_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (v_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=375, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import charactertokenizer\n",
    "\n",
    "# model_dir = 'ai-forever/charllama-2.6B'\n",
    "model_dir = 'ai-forever/charllama-1.3B'\n",
    "\n",
    "tokenizer = charactertokenizer.CharacterTokenizer.from_pretrained(model_dir)\n",
    "# print(tokenizer.get_vocab().keys())\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# save_dir = './charllama_tokenizer'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Save tokenizer files\n",
    "# tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# # Load tokenizer config (contains char_ords)\n",
    "# config_path = os.path.join(save_dir, 'tokenizer_config.json')\n",
    "# with open(config_path, 'r', encoding='utf-8') as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "# # Extract char ordinals\n",
    "# char_ords = config.get(\"char_ords\", [])\n",
    "\n",
    "# # Convert to readable characters\n",
    "# char_list = []\n",
    "# for c in char_ords:\n",
    "#     try:\n",
    "#         char_list.append({'ord': c, 'char': chr(c)})\n",
    "#     except:\n",
    "#         char_list.append({'ord': c, 'char': '<Invalid>'})\n",
    "\n",
    "# # Save as JSON\n",
    "# with open(os.path.join(save_dir, 'vocab_chars.json'), 'w', encoding='utf-8') as f:\n",
    "#     json.dump(char_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# # Save as plain text\n",
    "# with open(os.path.join(save_dir, 'vocab_chars.txt'), 'w', encoding='utf-8') as f:\n",
    "#     for item in char_list:\n",
    "#         f.write(f\"{item['ord']}: {item['char']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " '<sep>': 4,\n",
       " '<cls>': 5,\n",
       " '<mask>': 6,\n",
       " '\\x08': 7,\n",
       " 'З': 8,\n",
       " 'а': 9,\n",
       " ' ': 10,\n",
       " 'о': 11,\n",
       " 'к': 12,\n",
       " 'н': 13,\n",
       " '́': 14,\n",
       " 'м': 15,\n",
       " 'п': 16,\n",
       " 'р': 17,\n",
       " 'с': 18,\n",
       " 'е': 19,\n",
       " 'л': 20,\n",
       " 'и': 21,\n",
       " 'я': 22,\n",
       " 'б': 23,\n",
       " 'ы': 24,\n",
       " ',': 25,\n",
       " '\\n': 26,\n",
       " 'ч': 27,\n",
       " 'т': 28,\n",
       " 'ь': 29,\n",
       " 'г': 30,\n",
       " '.': 31,\n",
       " 'В': 32,\n",
       " 'в': 33,\n",
       " 'ё': 34,\n",
       " 'з': 35,\n",
       " 'Н': 36,\n",
       " 'у': 37,\n",
       " 'ю': 38,\n",
       " 'д': 39,\n",
       " 'Я': 40,\n",
       " 'х': 41,\n",
       " 'ш': 42,\n",
       " 'й': 43,\n",
       " 'Э': 44,\n",
       " 'К': 45,\n",
       " 'Б': 46,\n",
       " 'щ': 47,\n",
       " 'О': 48,\n",
       " 'С': 49,\n",
       " 'Д': 50,\n",
       " 'э': 51,\n",
       " '-': 52,\n",
       " 'ж': 53,\n",
       " 'ц': 54,\n",
       " 'И': 55,\n",
       " 'П': 56,\n",
       " 'Ч': 57,\n",
       " 'А': 58,\n",
       " 'Т': 59,\n",
       " '\\x0c': 60,\n",
       " '\\x0b': 61,\n",
       " 'Р': 62,\n",
       " '2': 63,\n",
       " '0': 64,\n",
       " '8': 65,\n",
       " '9': 66,\n",
       " '4': 67,\n",
       " '3': 68,\n",
       " '%': 69,\n",
       " '7': 70,\n",
       " '(': 71,\n",
       " 'ф': 72,\n",
       " ')': 73,\n",
       " '1': 74,\n",
       " 'М': 75,\n",
       " '!': 76,\n",
       " 'Ц': 77,\n",
       " '5': 78,\n",
       " 'ъ': 79,\n",
       " '6': 80,\n",
       " '$': 81,\n",
       " 'Л': 82,\n",
       " ':': 83,\n",
       " '«': 84,\n",
       " '»': 85,\n",
       " 'У': 86,\n",
       " 'Х': 87,\n",
       " '?': 88,\n",
       " 'Г': 89,\n",
       " 'Ф': 90,\n",
       " 'Ж': 91,\n",
       " 'Ш': 92,\n",
       " 'Е': 93,\n",
       " '\"': 94,\n",
       " '…': 95,\n",
       " 'Ё': 96,\n",
       " ';': 97,\n",
       " 'Щ': 98,\n",
       " '„': 99,\n",
       " '“': 100,\n",
       " 'Ю': 101,\n",
       " '№': 102,\n",
       " '§': 103,\n",
       " '/': 104,\n",
       " 'Ы': 105,\n",
       " '’': 106,\n",
       " 'Й': 107,\n",
       " 'Ь': 108,\n",
       " \"'\": 109,\n",
       " ']': 110,\n",
       " '*': 111,\n",
       " '[': 112,\n",
       " '<': 113,\n",
       " '>': 114,\n",
       " 'Ъ': 115,\n",
       " '”': 116,\n",
       " '+': 117,\n",
       " '±': 118,\n",
       " '=': 119,\n",
       " 'क': 120,\n",
       " 'ल': 121,\n",
       " 'ह': 122,\n",
       " 'ठ': 123,\n",
       " 'य': 124,\n",
       " 'ो': 125,\n",
       " 'ग': 126,\n",
       " 'प': 127,\n",
       " '्': 128,\n",
       " 'र': 129,\n",
       " 'द': 130,\n",
       " 'ी': 131,\n",
       " 'ि': 132,\n",
       " 'ा': 133,\n",
       " 'é': 134,\n",
       " 'ó': 135,\n",
       " '\\\\': 136,\n",
       " '×': 137,\n",
       " '̆': 138,\n",
       " '€': 139,\n",
       " 'ú': 140,\n",
       " '&': 141,\n",
       " 'Ј': 142,\n",
       " '¦': 143,\n",
       " '‹': 144,\n",
       " '°': 145,\n",
       " 'ᰴ': 146,\n",
       " '⬓': 147,\n",
       " '›': 148,\n",
       " 'ј': 149,\n",
       " 'љ': 150,\n",
       " '�': 151,\n",
       " 'ý': 152,\n",
       " '\\x0f': 153,\n",
       " '´': 154,\n",
       " 'ќ': 155,\n",
       " 'T': 156,\n",
       " 'e': 157,\n",
       " 'Q': 158,\n",
       " 'u': 159,\n",
       " 'i': 160,\n",
       " 'r': 161,\n",
       " 'o': 162,\n",
       " 'c': 163,\n",
       " 'R': 164,\n",
       " '{': 165,\n",
       " '}': 166,\n",
       " '@': 167,\n",
       " 'b': 168,\n",
       " 'n': 169,\n",
       " 'P': 170,\n",
       " 's': 171,\n",
       " 'S': 172,\n",
       " 'O': 173,\n",
       " 'C': 174,\n",
       " 'G': 175,\n",
       " 'l': 176,\n",
       " 'f': 177,\n",
       " 'd': 178,\n",
       " 'm': 179,\n",
       " 'a': 180,\n",
       " 'g': 181,\n",
       " 'I': 182,\n",
       " 'v': 183,\n",
       " 'y': 184,\n",
       " 'E': 185,\n",
       " 't': 186,\n",
       " 'h': 187,\n",
       " 'D': 188,\n",
       " 'p': 189,\n",
       " '#': 190,\n",
       " 'N': 191,\n",
       " '^': 192,\n",
       " 'Z': 193,\n",
       " 'A': 194,\n",
       " 'L': 195,\n",
       " 'M': 196,\n",
       " 'B': 197,\n",
       " 'V': 198,\n",
       " 'w': 199,\n",
       " 'k': 200,\n",
       " 'Y': 201,\n",
       " 'X': 202,\n",
       " 'F': 203,\n",
       " '\\x05': 204,\n",
       " 'x': 205,\n",
       " 'H': 206,\n",
       " 'j': 207,\n",
       " 'U': 208,\n",
       " 'q': 209,\n",
       " 'і': 210,\n",
       " 'á': 211,\n",
       " 'K': 212,\n",
       " 'W': 213,\n",
       " 'І': 214,\n",
       " 'z': 215,\n",
       " 'ѕ': 216,\n",
       " 'J': 217,\n",
       " 'º': 218,\n",
       " 'ў': 219,\n",
       " 'Ў': 220,\n",
       " '\\x1a': 221,\n",
       " 'ͣ': 222,\n",
       " 'ͨ': 223,\n",
       " 'њ': 224,\n",
       " 'ћ': 225,\n",
       " 'ѓ': 226,\n",
       " 'Ќ': 227,\n",
       " 'Ѕ': 228,\n",
       " '̀': 229,\n",
       " '\\x10': 230,\n",
       " '′': 231,\n",
       " 'ѝ': 232,\n",
       " '‐': 233,\n",
       " '£': 234,\n",
       " '¼': 235,\n",
       " '\\u3000': 236,\n",
       " '¹': 237,\n",
       " '²': 238,\n",
       " '³': 239,\n",
       " 'Џ': 240,\n",
       " '—': 241,\n",
       " '–': 242,\n",
       " '¤': 243,\n",
       " '¢': 244,\n",
       " '`': 245,\n",
       " '\\u202c': 246,\n",
       " '―': 247,\n",
       " '−': 248,\n",
       " '|': 249,\n",
       " '¶': 250,\n",
       " '‚': 251,\n",
       " '\\x11': 252,\n",
       " 'ʼ': 253,\n",
       " '\\u202d': 254,\n",
       " '®': 255,\n",
       " '¥': 256,\n",
       " '┴': 257,\n",
       " '·': 258,\n",
       " '\\x98': 259,\n",
       " '\\uf00a': 260,\n",
       " '̧': 261,\n",
       " 'ḿ': 262,\n",
       " 'ˆ': 263,\n",
       " 'Κ': 264,\n",
       " 'α': 265,\n",
       " 'λ': 266,\n",
       " 'η': 267,\n",
       " 'μ': 268,\n",
       " 'ε': 269,\n",
       " 'ρ': 270,\n",
       " 'ĸ': 271,\n",
       " '\\ufeff': 272,\n",
       " '∙': 273,\n",
       " 'қ': 274,\n",
       " '▵': 275,\n",
       " 'ꐦ': 276,\n",
       " 'ಠ': 277,\n",
       " '〈': 278,\n",
       " '〉': 279,\n",
       " '‖': 280,\n",
       " '䌌': 281,\n",
       " 'ʹ': 282,\n",
       " 'ђ': 283,\n",
       " '═': 284,\n",
       " '╛': 285,\n",
       " 'Ѳ': 286,\n",
       " 'ѳ': 287,\n",
       " 'Љ': 288,\n",
       " '†': 289,\n",
       " '\\x07': 290,\n",
       " 'џ': 291,\n",
       " '‒': 292,\n",
       " '≥': 293,\n",
       " 'ˮ': 294,\n",
       " '\\uf0a2': 295,\n",
       " 'ק': 296,\n",
       " 'ר': 297,\n",
       " 'ב': 298,\n",
       " 'נ': 299,\n",
       " '\\uf0bc': 300,\n",
       " '◦': 301,\n",
       " '\\x18': 302,\n",
       " 'ї': 303,\n",
       " '╕': 304,\n",
       " '╢': 305,\n",
       " '╤': 306,\n",
       " '√': 307,\n",
       " '┘': 308,\n",
       " '╓': 309,\n",
       " '⌠': 310,\n",
       " '■': 311,\n",
       " '╥': 312,\n",
       " '╖': 313,\n",
       " '▌': 314,\n",
       " 'ό': 315,\n",
       " 'à': 316,\n",
       " '″': 317,\n",
       " '▒': 318,\n",
       " '▓': 319,\n",
       " '╬': 320,\n",
       " 'َ': 321,\n",
       " '¸': 322,\n",
       " 'ë': 323,\n",
       " '🤬': 324,\n",
       " 'ė': 325,\n",
       " 'ә': 326,\n",
       " 'ң': 327,\n",
       " 'ө': 328,\n",
       " 'ү': 329,\n",
       " '☼': 330,\n",
       " '≪': 331,\n",
       " '≫': 332,\n",
       " '⠀': 333,\n",
       " '❄': 334,\n",
       " '️': 335,\n",
       " 'ۥ': 336,\n",
       " 'è': 337,\n",
       " 'Μ': 338,\n",
       " 'κ': 339,\n",
       " 'δ': 340,\n",
       " 'ο': 341,\n",
       " 'ν': 342,\n",
       " 'ί': 343,\n",
       " 'ò': 344,\n",
       " '《': 345,\n",
       " '》': 346,\n",
       " '̶': 347,\n",
       " '！': 348,\n",
       " '̈': 349,\n",
       " '́': 350,\n",
       " '✍': 351,\n",
       " 'Ó': 352,\n",
       " '（': 353,\n",
       " '）': 354,\n",
       " 'ŷ': 355,\n",
       " 'ҡ': 356,\n",
       " 'ҙ': 357,\n",
       " 'ғ': 358,\n",
       " '˗': 359,\n",
       " 'ά': 360,\n",
       " 'ѿ': 361,\n",
       " '\\x8d': 362,\n",
       " '\\uf008': 363,\n",
       " 'ỳ': 364,\n",
       " '\\ue02d': 365,\n",
       " 'Χ': 366,\n",
       " 'ؘ': 367,\n",
       " '❤': 368,\n",
       " 'һ': 369,\n",
       " 'ҫ': 370,\n",
       " 'Ә': 371,\n",
       " '苦': 372,\n",
       " '無': 373,\n",
       " 'π': 374}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEMANT IS MY NAME\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 3.6717\n",
      "Perplexity: 39.3203\n",
      "\n",
      "HEEEMMMANNNNT IIIIS MMMMY N NAAAME EE\n",
      "Cross-entropy loss: 3.8919\n",
      "Perplexity: 49.0039\n",
      "\n",
      "DRNOT TL KTCRNT ST RCT S C CDR C T DGT NE\n",
      "Cross-entropy loss: 3.4540\n",
      "Perplexity: 31.6276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for text in input_texts:\n",
    "        # Poetry completion\n",
    "        prompt =  text \n",
    "        print(prompt)\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        input_ids = inputs['input_ids']\n",
    "\n",
    "        # # Decode input IDs back to string\n",
    "        # decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "        # print(\"Decoded text from input_ids:\", repr(decoded_text))\n",
    "\n",
    "        # # Optional: Show individual tokens\n",
    "        # tokens = [tokenizer.decode([tid.item()]) for tid in input_ids[0]]\n",
    "        # print(\"\\nIndividual decoded tokens:\")\n",
    "        # for i, token in enumerate(tokens):\n",
    "        #     print(f\"Token {i}: {repr(token)} (id={input_ids[0][i].item()})\")\n",
    "            \n",
    "        # Forward pass to get logits\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits  # shape: (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        # Shift logits and labels for causal loss\n",
    "        # Predicted logits are for tokens [0 ... n-2], targets are [1 ... n-1]\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "        # Flatten the tensors for loss calculation\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                        shift_labels.view(-1))\n",
    "\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        print(f\"Cross-entropy loss: {loss.item():.4f}\")\n",
    "        print(f\"Perplexity: {perplexity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/281241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281241/281241 [00:00<00:00, 1498635.22it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/rajivratn/anaconda3/envs/langspeech/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/home/rajivratn/anaconda3/envs/langspeech/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2669075/204741776.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/70263 [00:00<?, ?it/s]/tmp/ipykernel_2669075/204741776.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/3:   0%|          | 178/70263 [00:25<2:13:30,  8.75it/s, loss=2.2187, lr=0.000000, step=6]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import charactertokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "# --- Config ---\n",
    "model_dir = 'ai-forever/charllama-1.3B'\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 4\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "max_length = 256\n",
    "gradient_accumulation_steps = 32\n",
    "save_steps = 5000\n",
    "log_interval = 10  # Log every 10 iterations\n",
    "output_dir = \"charllama-finetuned\"  # Directory to save checkpoints\n",
    "log_dir = os.path.join(\"runs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# --- Dataset class ---\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sentences = self._load_and_preprocess(file_path)\n",
    "\n",
    "    def _load_and_preprocess(self, file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        return [line.strip().upper() for line in lines if len(line.strip()) > 10]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tokens = self.tokenizer(sentence,\n",
    "                                max_length=self.max_length,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_tensors='pt')\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# --- Load tokenizer ---\n",
    "tokenizer = charactertokenizer.CharacterTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# --- Load model, optimizer, scheduler, and scaler from checkpoint (if available) ---\n",
    "checkpoint_path = None  # Set to the checkpoint directory if you want to resume training\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_dir).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,  # Will be updated if loading from checkpoint\n",
    "    num_training_steps=1 # Initialize with a non-zero value to avoid the check if loading\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "global_step = 0\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    checkpoint_dirs = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d)) and \"checkpoint-step-\" in d]\n",
    "    if checkpoint_dirs:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split('-')[-1]))\n",
    "        checkpoint_path = latest_checkpoint\n",
    "        print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(checkpoint_path).to(device)\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(checkpoint_path, 'optimizer.pt')))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(checkpoint_path, 'scheduler.pt')))\n",
    "        scaler.load_state_dict(torch.load(os.path.join(checkpoint_path, 'scaler.pt')))\n",
    "        global_step = int(checkpoint_path.split('-')[-1])\n",
    "        train_dataset_temp = CharDataset(\"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd\", tokenizer, max_length)\n",
    "        start_epoch = global_step // len(DataLoader(train_dataset_temp, batch_size=batch_size))\n",
    "        print(f\"Resuming from global step: {global_step}, epoch: {start_epoch}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found. Starting training from scratch.\")\n",
    "        train_dataset_temp = CharDataset(\"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd\", tokenizer, max_length)\n",
    "        scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=len(DataLoader(train_dataset_temp, batch_size=batch_size)) // 10,\n",
    "            num_training_steps=len(DataLoader(train_dataset_temp, batch_size=batch_size)) * num_epochs // gradient_accumulation_steps\n",
    "        )\n",
    "else:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Starting training from scratch.\")\n",
    "    train_dataset_temp = CharDataset(\"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd\", tokenizer, max_length)\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=len(DataLoader(train_dataset_temp, batch_size=batch_size)) // 10,\n",
    "        num_training_steps=len(DataLoader(train_dataset_temp, batch_size=batch_size)) * num_epochs // gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "model.train()\n",
    "\n",
    "# --- Create dataset and dataloader ---\n",
    "train_file_path = \"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd\"\n",
    "dataset = CharDataset(train_file_path, tokenizer, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Training Loop ---\n",
    "model.zero_grad()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), initial=global_step % len(dataloader) if start_epoch == epoch else 0, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for step, batch in progress_bar:\n",
    "        if start_epoch == epoch and step < global_step % len(dataloader):\n",
    "            continue  # Skip steps already done in the previous run\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            # Log loss and learning rate to TensorBoard every log_interval steps\n",
    "            if global_step % log_interval == 0:\n",
    "                writer.add_scalar('loss/step', loss.item() * gradient_accumulation_steps, global_step)\n",
    "                writer.add_scalar('learning_rate', scheduler.get_last_lr()[0], global_step)\n",
    "\n",
    "            if global_step % save_steps == 0:\n",
    "                checkpoint_dir = os.path.join(output_dir, f\"checkpoint-step-{global_step}\")\n",
    "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                model.save_pretrained(checkpoint_dir)\n",
    "                tokenizer.save_pretrained(checkpoint_dir)\n",
    "                torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, 'optimizer.pt'))\n",
    "                torch.save(scheduler.state_dict(), os.path.join(checkpoint_dir, 'scheduler.pt'))\n",
    "                torch.save(scaler.state_dict(), os.path.join(checkpoint_dir, 'scaler.pt'))\n",
    "                print(f\"Checkpoint saved at step {global_step} to {checkpoint_dir}\")\n",
    "\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        progress_bar.set_postfix({\"loss\": f\"{total_loss / (step + 1):.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.6f}\", \"step\": global_step + 1})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")\n",
    "    writer.add_scalar('loss/epoch', avg_loss, epoch + 1)\n",
    "\n",
    "# --- Save the final trained model ---\n",
    "final_output_dir = os.path.join(output_dir, \"final-model\")\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "model.save_pretrained(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "print(f\"Final model saved to {final_output_dir}\")\n",
    "\n",
    "# --- Close TensorBoard writer ---\n",
    "writer.close()\n",
    "print(f\"TensorBoard logs saved to {log_dir}\")\n",
    "print(\"To view TensorBoard logs, run: `tensorboard --logdir runs` from your terminal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
