{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 2.75GB [02:03, 22.3MB/s]\n",
      "Generating train split: 100%|██████████| 13100/13100 [00:00<00:00, 23608.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"keithito/lj_speech\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'audio', 'file', 'text', 'normalized_text'],\n",
       "        num_rows: 13100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 'LJ001-0001',\n",
       "  'audio': {'path': '/raid/home/rajivratn/.cache/huggingface/datasets/downloads/extracted/a19e32395d1e4b005ff54cae9043ff76873b8134eaa4bfbd92c9105a9b91e46a/LJSpeech-1.1/wavs/LJ001-0001.wav',\n",
       "   'array': array([-7.32421875e-04, -7.62939453e-04, -6.40869141e-04, ...,\n",
       "           7.32421875e-04,  2.13623047e-04,  6.10351562e-05]),\n",
       "   'sampling_rate': 22050},\n",
       "  'file': '/raid/home/rajivratn/.cache/huggingface/datasets/downloads/extracted/a19e32395d1e4b005ff54cae9043ff76873b8134eaa4bfbd92c9105a9b91e46a/LJSpeech-1.1/wavs/LJ001-0001.wav',\n",
       "  'text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
       "  'normalized_text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'},\n",
       " {'path': '/raid/home/rajivratn/.cache/huggingface/datasets/downloads/extracted/a19e32395d1e4b005ff54cae9043ff76873b8134eaa4bfbd92c9105a9b91e46a/LJSpeech-1.1/wavs/LJ001-0001.wav',\n",
       "  'array': array([-7.32421875e-04, -7.62939453e-04, -6.40869141e-04, ...,\n",
       "          7.32421875e-04,  2.13623047e-04,  6.10351562e-05]),\n",
       "  'sampling_rate': 22050},\n",
       " 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "audio = example['audio']\n",
    "transcription = example['text']\n",
    "\n",
    "example, audio, transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speakers: 585\n",
      "Total samples: 24473, Total speakers: 585\n"
     ]
    }
   ],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        input_manifest = \"/raid/home/rajivratn/hemant_rajivratn/librispeech/data/manifest/train-clean-100.tsv\"\n",
    "\n",
    "        # Read the first line to get the root directory\n",
    "        with open(input_manifest, \"r\") as infile:\n",
    "            root_dir = infile.readline().strip()  # First line is the root directory\n",
    "\n",
    "        # Define valid duration range\n",
    "        min_duration = 32000  # 2 seconds\n",
    "        max_duration = 250000  # 15.625 seconds\n",
    "\n",
    "        # Dictionary to store filtered samples per speaker\n",
    "        filtered_samples_by_speaker = {}\n",
    "\n",
    "        with open(input_manifest, \"r\") as infile:\n",
    "            infile.readline()  # Skip header (already read root_dir)\n",
    "            for line in infile:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                file_name, duration = parts\n",
    "                duration = int(duration)\n",
    "\n",
    "                if min_duration <= duration <= max_duration:\n",
    "                    full_path = os.path.join(root_dir, file_name)\n",
    "                    speaker_id = file_name.split(\"_\")[1]  # Extract speaker ID\n",
    "                    \n",
    "                    if speaker_id not in filtered_samples_by_speaker:\n",
    "                        filtered_samples_by_speaker[speaker_id] = []\n",
    "                    \n",
    "                    filtered_samples_by_speaker[speaker_id].append((full_path, duration))\n",
    "\n",
    "        self.diff_speakers = len(filtered_samples_by_speaker)\n",
    "        print(f\"Total speakers: {self.diff_speakers}\")\n",
    "        # a tuple with path, speaker, duration\n",
    "        filtered_samples = []\n",
    "        count = 0\n",
    "        for k in filtered_samples_by_speaker:\n",
    "            count += 1\n",
    "            for i in filtered_samples_by_speaker[k]:\n",
    "                filtered_samples.append((i[0], count, i[1]))\n",
    "            #     break\n",
    "            # if len(filtered_samples) == 40: \n",
    "            # break\n",
    "            \n",
    "        print(f\"Total samples: {len(filtered_samples)}, Total speakers: {count}\")\n",
    "        # Sort by duration\n",
    "        filtered_samples.sort(key=lambda x: x[-1])\n",
    "\n",
    "        self.dataset = filtered_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, speaker, duration = self.dataset[idx]\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        assert sample_rate == 16000, \"Sampling rate must be 16000\"\n",
    "        return waveform.squeeze(0), speaker, duration\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = AudioDataset()\n",
    "\n",
    "# create a collate function to truncate the audio files to minimum length\n",
    "def collate_fn(batches):\n",
    "    min_dur = min([batch[2] for batch in batches])\n",
    "    waveforms = []\n",
    "    speakers = []\n",
    "    for batch in batches:\n",
    "        waveforms.append(batch[0][:min_dur])\n",
    "        speakers.append(batch[1])\n",
    "    return torch.stack(waveforms), torch.tensor(speakers).unsqueeze(1) # bsz, seq_len and bsz, 1\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32080]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader: \n",
    "    print(i[0].shape, i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder, Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder() \n",
    "downsampling = Downsampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import models\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mencoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Encoder, Downsampling\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FrozenVocabulary, get_closest_vocab, merge_similar_indices\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdecoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Upsampling, Decoder, calculate_params\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcodec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Codec\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vocab'"
     ]
    }
   ],
   "source": [
    "# import models\n",
    "from encoder import Encoder, Downsampling\n",
    "from vocab import FrozenVocabulary, get_closest_vocab, merge_similar_indices\n",
    "from decoder import Upsampling, Decoder, calculate_params\n",
    "from codec import Codec\n",
    "\n",
    "print(\"All imports are successful\")\n",
    "\n",
    "class Spk_Embed(nn.Module):\n",
    "    def __init__(self, num_speakers=100, spk_embed_dim=256):\n",
    "        super(Spk_Embed, self).__init__()\n",
    "        self.spk_embed = nn.Embedding(num_speakers, spk_embed_dim)\n",
    "        \n",
    "    def forward(self, speaker):\n",
    "        return self.spk_embed(speaker)\n",
    "    \n",
    "# params\n",
    "hidden_dim = 256\n",
    "spk_embed_dim = 256\n",
    "num_speakers = dataset.diff_speakers\n",
    "\n",
    "# models \n",
    "spk_embed = Spk_Embed(num_speakers=num_speakers, spk_embed_dim=spk_embed_dim)\n",
    "encoder = Encoder() # frozen\n",
    "downsampling = Downsampling()\n",
    "vocab = FrozenVocabulary(path=\"vocab.pth\") # frozen\n",
    "upsampling = Upsampling(inp_dim=int(768+spk_embed_dim), hidden_dim=hidden_dim)\n",
    "decoder = Decoder(hidden_dim=hidden_dim, out_dim=1024, num_blocks=5, kernel_size=11)\n",
    "codec = Codec() # frozen\n",
    "vocab_embeddings, char_to_idx, idx_to_char = vocab.embeddings, vocab.char_to_idx, vocab.idx_to_char\n",
    "\n",
    "print(idx_to_char)\n",
    "print(f\"Paraeters of spk_embed: {calculate_params(spk_embed)}\")\n",
    "print(f\"Paraeters of downsampling: {calculate_params(downsampling)}\")\n",
    "print(f\"Paraeters of upsampling: {calculate_params(upsampling)}\")\n",
    "print(f\"Paraeters of decoder: {calculate_params(decoder)}\")\n",
    "\n",
    "print(\"Models are initialized\")\n",
    "\n",
    "\n",
    "# Set the models to gpu\n",
    "device = torch.device(\"cuda\")\n",
    "encoder = encoder.to(device)\n",
    "downsampling = downsampling.to(device)\n",
    "vocab_embeddings = vocab_embeddings.to(device)\n",
    "decoder = decoder.to(device)\n",
    "upsampling = upsampling.to(device)\n",
    "codec.model = codec.model.to(device)\n",
    "spk_embed = spk_embed.to(device)\n",
    "\n",
    "# freeze the encoder, and codec\n",
    "for param in codec.model.parameters():\n",
    "    param.requires_grad = False   \n",
    "vocab_embeddings.requires_grad = False\n",
    "\n",
    "# Training loop\n",
    "downsampling.train()\n",
    "decoder.train()\n",
    "upsampling.train()\n",
    "codec.model.eval()\n",
    "spk_embed.train()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the models to training mode\n",
    "encoder.train()\n",
    "for param in encoder.named_parameters():\n",
    "    param[1].requires_grad = False\n",
    "    continue\n",
    "    if \"model.encoder.layers.8\" in param[0] or \"model.encoder.layers.11\" in param[0]:\n",
    "        param[1].requires_grad = True\n",
    "    else:\n",
    "        param[1].requires_grad = False\n",
    "        \n",
    "optimizer = optim.Adam(\n",
    "    list(downsampling.parameters()) + list(decoder.parameters()) + list(upsampling.parameters()) + list(spk_embed.parameters()),\n",
    "    # list(downsampling.parameters()) + list(decoder.parameters()) + list(upsampling.parameters()) + list(encoder.parameters()) + list(spk_embed.parameters()),\n",
    "    lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 66]' is invalid for input of size 6336",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m downsampling_output \u001b[38;5;241m=\u001b[39m downsampling(encoder_output) \u001b[38;5;66;03m# torch.Size([32, 768, 172])\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Get the closest vocab embeddings\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m commitment_loss, vocab_output, indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_closest_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownsampling_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# add speaker embeddings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m speaker \u001b[38;5;241m=\u001b[39m spk_embed(speaker)\n",
      "File \u001b[0;32m/raid/home/rajivratn/hemant_rajivratn/last/utils/vocab.py:41\u001b[0m, in \u001b[0;36mget_closest_vocab\u001b[0;34m(z, e)\u001b[0m\n\u001b[1;32m     38\u001b[0m commitment_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(z, z_q\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m     39\u001b[0m z_q \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m (z_q \u001b[38;5;241m-\u001b[39m z)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m commitment_loss, z_q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[43mmin_encoding_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 66]' is invalid for input of size 6336"
     ]
    }
   ],
   "source": [
    "def merge_tensors(t1, t2):\n",
    "    t2 = t2.unsqueeze(-1)  # Reshape to (batch, features, 1)\n",
    "    return torch.cat([t1, t2.expand(-1, -1, t1.shape[-1])], dim=1)\n",
    "\n",
    "# start training\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    running_loss = 0.0\n",
    "    for iteration, data in enumerate(dataloader):\n",
    "        # data\n",
    "        waveform, speaker = data\n",
    "        waveform = waveform.to(device) \n",
    "        speaker = torch.tensor(speaker).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            encoder_output = encoder(waveform)\n",
    "        downsampling_output = downsampling(encoder_output) # torch.Size([32, 768, 172])\n",
    "        # Get the closest vocab embeddings\n",
    "        commitment_loss, vocab_output, indices = get_closest_vocab(downsampling_output, vocab_embeddings)\n",
    "        \n",
    "        # add speaker embeddings\n",
    "        speaker = spk_embed(speaker)\n",
    "        vocab_output = merge_tensors(vocab_output, speaker)\n",
    "\n",
    "        \n",
    "        # Upsampling\n",
    "        upsampling_output = upsampling(vocab_output)\n",
    "        # Decoder\n",
    "        decoder_output = decoder(upsampling_output).contiguous() # torch.Size([32, 1024, 172])\n",
    "        \n",
    "        # Codec\n",
    "        with torch.no_grad():\n",
    "            codec_output = codec.encode(waveform).detach().contiguous()\n",
    "        \n",
    "        # Ensure same sequence length for ground truth and output\n",
    "        min_seq_len = min(codec_output.shape[-1], decoder_output.shape[-1])    \n",
    "        codec_output = codec_output[:, :, :min_seq_len]\n",
    "        decoder_output = decoder_output[:, :, :min_seq_len]    \n",
    "\n",
    "        # Compute the loss\n",
    "        l2_loss = F.mse_loss(decoder_output, codec_output)\n",
    "        # commitment_loss *= 10\n",
    "        loss =  l2_loss + commitment_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # empty cache\n",
    "        torch.cuda.empty_cache()  \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # print for every 10 iterations\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Indices: {indices[0]}\")\n",
    "            print(f\"Epoch: {epoch}, Iteration: {iteration}/{len(dataloader)}, Loss: {running_loss/(iteration+1)}, commit_loss: {commitment_loss.item()}, l2_loss: {l2_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 768, 66]), torch.Size([29, 256]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampling_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = indices\n",
    "\"\".join([idx_to_char[i] for i in merge_similar_indices(ind)[0]]) #.replace(\"<sil>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode audio signal\n",
    "y = codec.model.decode(decoder_output[1:2,:,:]).cpu().detach().numpy()\n",
    "\n",
    "# play the numpy array as audio using ipython.display.Audio\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(y[0,0,:], rate=16000)  # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(waveform[1,:].cpu().detach().numpy(), rate=16000)  # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
