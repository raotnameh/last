checkpoint:
  step: 10000
codebook:
  model_name: meta-llama/Llama-3.2-1B
dataset_speech:
  batch_size: 2
  max_duration: 160000
  min_duration: 32000
  test_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/test.tsv
  train_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/train.tsv
  val_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/val.tsv
dataset_txt:
  batch_size: 2
  path: /raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd
  skip_non_speech: true
device: cuda
downsample:
  groups: 16
  kernel_size: 5
  stride: 1
encoder:
  ckpt_path: /raid/home/rajivratn/hemant_rajivratn/last/weights/hubert_base_ls960.pt
  encoder_embed_dim: 768
  frozen_layers:
  - '11'
logging:
  dir: /raid/home/rajivratn/hemant_rajivratn/grpo/src/logs2
  step: 1
lr_scheduler:
  phase_ratio:
  - 0.01
  - 0.0
  - 0.99
train:
  accumulation_steps: 8
  epochs: 10
  epsilon: 0.2
  freeze_epochs: 1
  grad_clip: 10.0
  groups: 64
  iters: 2
  lr: 0.0001
  resume_checkpoint: false
  resume_path: checkpoints/step_10000.pth
  seed: 42123
  temp: 2
