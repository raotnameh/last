checkpoint:
  step: 10000
codebook:
  model_name: meta-llama/Llama-3.2-1B-Instruct
dataset_speech:
  batch_size: 4
  max_duration: 160000
  min_duration: 32000
  test_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/test.tsv
  train_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/train.tsv
  val_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/val.tsv
dataset_txt:
  batch_size: 4
  path: /raid/home/rajivratn/hemant_rajivratn/last/data/txt/train_norm.txt
  skip_non_speech: false
decoder:
  max_seq_len: 2048
  speaker:
    speaker_emb_dim: 512
    use_s: true
  transformer:
    conv_filter_size: 1024
    conv_kernel_size:
    - 9
    - 1
    dac_hidden: 1024
    decoder_dropout: 0.2
    decoder_head: 2
    decoder_hidden: 256
    decoder_layer: 4
device: cuda
discriminator:
  hidden_dim: 512
  kernel_size: 5
  num_layers: 10
downsample:
  groups: 64
  kernel_size: 11
  stride: 1
encoder:
  ckpt_path: ../weights/convert_iter3.pt
  encoder_embed_dim: 768
  frozen_layers:
  - '11'
eval:
  eval: false
logging:
  dir: logs_skip_non_speech_false
  step: 1
loss:
  commit_loss_weight: 1.0
  disc_loss_weight: 1.0
  gen_loss_weight: 1.0
  gp_weight: 1.0
  recon_loss_weight: 1.0
  smooth_loss_weight: 1.0
lr_scheduler:
  phase_ratio:
  - 0.2
  - 0.0
  - 0.8
train:
  discriminator_freq: 6
  freeze_steps: 1
  grad_clip: 1.0
  gradient_accumulation_steps: 1
  lr_dec: 0.0001
  lr_disc: 0.0001
  lr_down: 5.0e-05
  lr_enc: 5.0e-05
  mixed_precision: false
  num_steps: 100000
  resume_checkpoint: false
  resume_path: checkpoints/step_10000.pth
  seed: 42123
upsample:
  groups: 64
  kernel_size: 11
  stride: 1
