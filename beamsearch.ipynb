{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6f5a042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.jit.script\n",
    "def beam_search(log_probs: torch.Tensor, beam_size: int):\n",
    "    \"\"\"\n",
    "    Performs beam search on a tensor of log probabilities.\n",
    "\n",
    "    Args:\n",
    "        log_probs (torch.Tensor): Tensor of shape (b, t, v) containing log probabilities.\n",
    "        beam_size (int): Number of beams to keep at each time step.\n",
    "\n",
    "    Returns:\n",
    "        sequences (torch.Tensor): Tensor of shape (b, beam_size, t) containing the top sequences.\n",
    "        scores (torch.Tensor): Tensor of shape (b, beam_size) containing the scores of the top sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    b, t, v = log_probs.size()\n",
    "    \n",
    "    initial_beam_size = min(beam_size, v) # At the very first step (time step 0), we can't have more beams than the vocabulary size. This line ensures that the initial number of beams considered doesn't exceed the number of possible first tokens.\n",
    "\n",
    "    topk_scores, topk_indices = torch.topk(log_probs[:, 0, :], initial_beam_size, dim=-1) # Returns the k largest elements of the given input tensor along a given dimension\n",
    "    sequences = topk_indices.unsqueeze(-1)  # (b, initial_beam_size, 1)\n",
    "    scores = topk_scores  # (b, initial_beam_size)\n",
    "\n",
    "    for step in range(1, t):\n",
    "        # Expand the current sequences with all possible next tokens\n",
    "        current_log_probs = log_probs[:, step, :].unsqueeze(1)  # (b, 1, v)\n",
    "        expanded_scores = scores.unsqueeze(-1) + current_log_probs  # (b, beam_size, v)\n",
    "        flat_scores = expanded_scores.view(b, -1)  # (b, beam_size * v)\n",
    "\n",
    "        # Select the top-k scores and their corresponding indices\n",
    "        topk_flat_scores, topk_indices = flat_scores.topk(beam_size, dim=-1)  # (b, beam_size)\n",
    "        beam_indices = topk_indices // v  # Indices of sequences to expand\n",
    "        token_indices = topk_indices % v  # New tokens to append\n",
    "\n",
    "        # Gather the sequences to expand and append the new tokens\n",
    "        sequences = torch.gather(sequences, 1, beam_indices.unsqueeze(-1).expand(-1, -1, sequences.size(-1)))\n",
    "        sequences = torch.cat([sequences, token_indices.unsqueeze(-1)], dim=-1)  # (b, beam_size, step+1)\n",
    "\n",
    "        # Update the scores\n",
    "        scores = topk_flat_scores\n",
    "\n",
    "    return sequences, scores.unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0a06a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top sequences: tensor([[[ 3, 19, 19,  1, 11,  8,  8,  2,  4, 26, 27, 17,  7, 12, 21,  7,  9,\n",
      "           7, 15,  1, 25, 13, 25,  3, 13, 28, 25, 20, 24,  2,  8, 23, 26,  1,\n",
      "          22,  6, 22,  5, 12,  8, 10, 12, 24, 22, 28,  8, 17, 27,  6, 10,  6,\n",
      "          23, 25, 18, 22,  9,  0,  4, 10, 26,  4, 20,  5, 21, 17, 25, 19,  0,\n",
      "          11, 22, 10, 11, 15,  1, 25, 17,  9, 25, 23, 10, 22, 15,  3,  5,  1,\n",
      "          28, 12, 16, 28, 22, 22, 14, 18, 26, 17, 27,  1, 24, 19, 17],\n",
      "         [ 3, 19, 19,  1, 11,  8,  8,  2,  4, 26, 27, 17,  7, 12, 21,  7,  9,\n",
      "           7, 15,  1, 16, 13, 25,  3, 13, 28, 25, 20, 24,  2,  8, 23, 26,  1,\n",
      "          22,  6, 22,  5, 12,  8, 10, 12, 24, 22, 28,  8, 17, 27,  6, 10,  6,\n",
      "          23, 25, 18, 22,  9,  0,  4, 10, 26,  4, 20,  5, 21, 17, 25, 19,  0,\n",
      "          11, 22, 10, 11, 15,  1, 25, 17,  9, 25, 23, 10, 22, 15,  3,  5,  1,\n",
      "          28, 12, 16, 28, 22, 22, 14, 18, 26, 17, 27,  1, 24, 19, 17]],\n",
      "\n",
      "        [[21, 25, 13, 12,  9, 10,  6,  0, 23, 14,  5, 18,  9, 19, 10, 22, 14,\n",
      "          26,  2,  7,  3, 15,  5, 21,  5, 10, 25, 19, 18,  2, 22,  7, 25, 25,\n",
      "          18,  0,  1, 15,  7, 13,  3, 15,  5,  3, 17, 10, 24, 22, 15,  1,  4,\n",
      "           1, 23, 17,  5,  0, 12,  0, 18, 25, 15,  3, 10, 14, 26, 17, 20,  2,\n",
      "           2, 20,  1,  9,  3, 17,  8,  7,  2, 15, 27, 12, 26, 25, 15, 26, 10,\n",
      "          12, 16, 14, 22, 19,  3,  5,  8,  4, 14, 17,  9,  7, 12, 25],\n",
      "         [21, 25, 13, 12,  9, 10,  6,  0, 23, 14,  5, 18,  9, 19, 10, 22, 14,\n",
      "          26,  2,  7,  3, 15,  5, 21,  5, 10, 25, 19, 18,  2, 22,  7, 25, 25,\n",
      "          18,  0,  1, 15,  7, 13,  3, 15,  5,  3, 17, 10, 24, 22, 15,  1,  4,\n",
      "           1, 23, 17,  5,  0, 12,  5, 18, 25, 15,  3, 10, 14, 26, 17, 20,  2,\n",
      "           2, 20,  1,  9,  3, 17,  8,  7,  2, 15, 27, 12, 26, 25, 15, 26, 10,\n",
      "          12, 16, 14, 22, 19,  3,  5,  8,  4, 14, 17,  9,  7, 12, 25]]],\n",
      "       device='cuda:0')\n",
      "Scores: tensor([[[-187.0919],\n",
      "         [-187.0930]],\n",
      "\n",
      "        [[-183.3121],\n",
      "         [-183.3131]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 100\n",
    "vocab_size = 29\n",
    "beam_size = 2\n",
    "\n",
    "# Simulate log probabilities\n",
    "log_probs = torch.randn(batch_size, sequence_length, vocab_size).log_softmax(dim=-1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "log_probs = log_probs.to(device)\n",
    "\n",
    "# Perform beam search\n",
    "sequences, scores = beam_search(log_probs, beam_size)\n",
    "\n",
    "print(\"Top sequences:\", sequences) # bsz, beamsize,seq_len\n",
    "print(\"Scores:\", scores) # bsz, beamsize,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ecae46a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 100]), torch.Size([2, 2, 1]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "df9e0013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3404, -2.5064, -2.4116, -2.1333, -1.6098, -1.8109, -1.6270,\n",
       "          -2.2315, -1.5690, -2.1118, -2.1869, -1.9060, -2.2188, -1.4155,\n",
       "          -0.9839, -1.8076, -2.1683, -1.6998, -1.6267, -2.0404, -2.3530,\n",
       "          -2.4981, -1.0137, -1.8512, -2.0450, -1.8122, -1.8771, -2.1188,\n",
       "          -2.2018, -2.0547, -2.2928, -1.8956, -2.3741, -1.9083, -1.6760,\n",
       "          -2.1687, -1.5837, -1.8111, -1.9235, -2.1176, -1.9232, -1.8182,\n",
       "          -1.8214, -1.8918, -2.1509, -1.3543, -1.9072, -2.0839, -2.1099,\n",
       "          -1.4305, -1.7079, -2.2080, -2.1379, -2.0946, -1.8048, -2.0222,\n",
       "          -1.9320, -2.2414, -1.7560, -2.4706, -1.9054, -1.7765, -1.0247,\n",
       "          -1.3049, -2.0192, -2.2386, -2.2207, -1.6373, -1.0833, -1.5956,\n",
       "          -2.0073, -2.2533, -1.4145, -1.5185, -2.3554, -1.4078, -2.1353,\n",
       "          -2.3884, -1.4423, -1.0621, -2.0759, -1.6714, -1.6209, -1.6636,\n",
       "          -1.9561, -1.7320, -1.8773, -1.6799, -1.7005, -1.6985, -1.5929,\n",
       "          -2.0375, -1.2639, -1.8439, -2.3617, -1.6660, -1.4275, -1.5964,\n",
       "          -2.0597, -1.9260],\n",
       "         [-2.3404, -2.5064, -2.4116, -2.1333, -1.6098, -1.8109, -1.6270,\n",
       "          -2.2315, -1.5690, -2.1118, -2.1869, -1.9060, -2.2188, -1.4155,\n",
       "          -0.9839, -1.8076, -2.1683, -1.6998, -1.6267, -2.0404, -2.3542,\n",
       "          -2.4981, -1.0137, -1.8512, -2.0450, -1.8122, -1.8771, -2.1188,\n",
       "          -2.2018, -2.0547, -2.2928, -1.8956, -2.3741, -1.9083, -1.6760,\n",
       "          -2.1687, -1.5837, -1.8111, -1.9235, -2.1176, -1.9232, -1.8182,\n",
       "          -1.8214, -1.8918, -2.1509, -1.3543, -1.9072, -2.0839, -2.1099,\n",
       "          -1.4305, -1.7079, -2.2080, -2.1379, -2.0946, -1.8048, -2.0222,\n",
       "          -1.9320, -2.2414, -1.7560, -2.4706, -1.9054, -1.7765, -1.0247,\n",
       "          -1.3049, -2.0192, -2.2386, -2.2207, -1.6373, -1.0833, -1.5956,\n",
       "          -2.0073, -2.2533, -1.4145, -1.5185, -2.3554, -1.4078, -2.1353,\n",
       "          -2.3884, -1.4423, -1.0621, -2.0759, -1.6714, -1.6209, -1.6636,\n",
       "          -1.9561, -1.7320, -1.8773, -1.6799, -1.7005, -1.6985, -1.5929,\n",
       "          -2.0375, -1.2639, -1.8439, -2.3617, -1.6660, -1.4275, -1.5964,\n",
       "          -2.0597, -1.9260]],\n",
       "\n",
       "        [[-1.6530, -2.0240, -1.9034, -1.8869, -1.9554, -1.5995, -1.9430,\n",
       "          -2.2195, -1.8993, -2.2949, -1.5584, -1.9132, -1.8711, -1.8397,\n",
       "          -1.8954, -1.1685, -2.1497, -2.0013, -1.2655, -1.8072, -1.3327,\n",
       "          -0.9979, -0.9474, -1.7042, -1.7833, -2.2008, -1.9084, -1.2306,\n",
       "          -2.2425, -1.1792, -2.1187, -2.2731, -1.5651, -1.8466, -1.8573,\n",
       "          -1.2074, -2.2193, -2.1431, -2.1607, -2.1431, -1.5079, -1.7104,\n",
       "          -1.9866, -1.9978, -2.0345, -1.7489, -1.9997, -2.0462, -1.9101,\n",
       "          -1.8050, -1.7418, -2.0026, -2.1516, -2.0678, -1.4631, -1.9048,\n",
       "          -1.1947, -2.3164, -1.9320, -1.9929, -2.1242, -1.9972, -1.9332,\n",
       "          -1.7930, -2.5398, -2.3151, -2.0918, -2.0486, -1.6420, -1.5010,\n",
       "          -2.0154, -1.6628, -1.8775, -1.4762, -2.3606, -1.3898, -1.5552,\n",
       "          -1.9299, -1.8957, -1.6043, -1.7997, -2.1350, -1.9223, -1.4620,\n",
       "          -1.9986, -1.3879, -1.7956, -1.7371, -1.8294, -1.7073, -2.1564,\n",
       "          -1.9478, -2.0168, -2.1785, -1.4036, -2.1989, -0.8059, -1.8748,\n",
       "          -2.0826, -1.6935],\n",
       "         [-1.6530, -2.0240, -1.9034, -1.8869, -1.9554, -1.5995, -1.9430,\n",
       "          -2.2195, -1.8993, -2.2949, -1.5584, -1.9132, -1.8711, -1.8397,\n",
       "          -1.8954, -1.1685, -2.1497, -2.0013, -1.2655, -1.8072, -1.3327,\n",
       "          -0.9979, -0.9474, -1.7042, -1.7833, -2.2008, -1.9084, -1.2306,\n",
       "          -2.2425, -1.1792, -2.1187, -2.2731, -1.5651, -1.8466, -1.8573,\n",
       "          -1.2074, -2.2193, -2.1431, -2.1607, -2.1431, -1.5079, -1.7104,\n",
       "          -1.9866, -1.9978, -2.0345, -1.7489, -1.9997, -2.0462, -1.9101,\n",
       "          -1.8050, -1.7418, -2.0026, -2.1516, -2.0678, -1.4631, -1.9048,\n",
       "          -1.1947, -2.3174, -1.9320, -1.9929, -2.1242, -1.9972, -1.9332,\n",
       "          -1.7930, -2.5398, -2.3151, -2.0918, -2.0486, -1.6420, -1.5010,\n",
       "          -2.0154, -1.6628, -1.8775, -1.4762, -2.3606, -1.3898, -1.5552,\n",
       "          -1.9299, -1.8957, -1.6043, -1.7997, -2.1350, -1.9223, -1.4620,\n",
       "          -1.9986, -1.3879, -1.7956, -1.7371, -1.8294, -1.7073, -2.1564,\n",
       "          -1.9478, -2.0168, -2.1785, -1.4036, -2.1989, -0.8059, -1.8748,\n",
       "          -2.0826, -1.6935]]], device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_probs = torch.gather(log_probs, 2, sequences.transpose(1,2)).transpose(1,2) # bsz, beamsize, T\n",
    "path_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "76fefdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = scores.mean(dim=1, keepdim=True)\n",
    "std = scores.std(dim=1, keepdim=True)\n",
    "\n",
    "scores = (scores - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4aeda04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6549, -1.7723, -1.7052, -1.5085, -1.1383, -1.2805, -1.1505,\n",
       "          -1.5779, -1.1094, -1.4933, -1.5464, -1.3478, -1.5690, -1.0009,\n",
       "          -0.6957, -1.2781, -1.5332, -1.2019, -1.1503, -1.4428, -1.6639,\n",
       "          -1.7664, -0.7168, -1.3090, -1.4460, -1.2814, -1.3273, -1.4982,\n",
       "          -1.5569, -1.4529, -1.6213, -1.3404, -1.6787, -1.3494, -1.1851,\n",
       "          -1.5335, -1.1199, -1.2806, -1.3601, -1.4974, -1.3599, -1.2856,\n",
       "          -1.2879, -1.3377, -1.5209, -0.9576, -1.3486, -1.4735, -1.4919,\n",
       "          -1.0115, -1.2077, -1.5613, -1.5117, -1.4811, -1.2762, -1.4299,\n",
       "          -1.3661, -1.5849, -1.2416, -1.7470, -1.3473, -1.2562, -0.7246,\n",
       "          -0.9227, -1.4278, -1.5829, -1.5703, -1.1577, -0.7660, -1.1282,\n",
       "          -1.4194, -1.5934, -1.0002, -1.0738, -1.6655, -0.9955, -1.5099,\n",
       "          -1.6888, -1.0199, -0.7510, -1.4679, -1.1818, -1.1461, -1.1763,\n",
       "          -1.3832, -1.2247, -1.3275, -1.1879, -1.2024, -1.2010, -1.1264,\n",
       "          -1.4407, -0.8937, -1.3038, -1.6699, -1.1781, -1.0094, -1.1288,\n",
       "          -1.4564, -1.3619],\n",
       "         [ 1.6549,  1.7723,  1.7052,  1.5085,  1.1383,  1.2805,  1.1505,\n",
       "           1.5779,  1.1094,  1.4933,  1.5464,  1.3478,  1.5690,  1.0009,\n",
       "           0.6957,  1.2781,  1.5332,  1.2019,  1.1503,  1.4428,  1.6647,\n",
       "           1.7664,  0.7168,  1.3090,  1.4460,  1.2814,  1.3273,  1.4982,\n",
       "           1.5569,  1.4529,  1.6213,  1.3404,  1.6787,  1.3494,  1.1851,\n",
       "           1.5335,  1.1199,  1.2806,  1.3601,  1.4974,  1.3599,  1.2856,\n",
       "           1.2879,  1.3377,  1.5209,  0.9576,  1.3486,  1.4735,  1.4919,\n",
       "           1.0115,  1.2077,  1.5613,  1.5117,  1.4811,  1.2762,  1.4299,\n",
       "           1.3661,  1.5849,  1.2416,  1.7470,  1.3473,  1.2562,  0.7246,\n",
       "           0.9227,  1.4278,  1.5829,  1.5703,  1.1577,  0.7660,  1.1282,\n",
       "           1.4194,  1.5934,  1.0002,  1.0738,  1.6655,  0.9955,  1.5099,\n",
       "           1.6888,  1.0199,  0.7510,  1.4679,  1.1818,  1.1461,  1.1763,\n",
       "           1.3832,  1.2247,  1.3275,  1.1879,  1.2024,  1.2010,  1.1264,\n",
       "           1.4407,  0.8937,  1.3038,  1.6699,  1.1781,  1.0094,  1.1288,\n",
       "           1.4564,  1.3619]],\n",
       "\n",
       "        [[-1.1688, -1.4312, -1.3459, -1.3342, -1.3827, -1.1310, -1.3739,\n",
       "          -1.5694, -1.3430, -1.6227, -1.1020, -1.3528, -1.3231, -1.3009,\n",
       "          -1.3403, -0.8263, -1.5200, -1.4152, -0.8949, -1.2779, -0.9424,\n",
       "          -0.7056, -0.6699, -1.2051, -1.2610, -1.5562, -1.3494, -0.8702,\n",
       "          -1.5857, -0.8338, -1.4981, -1.6073, -1.1067, -1.3058, -1.3133,\n",
       "          -0.8537, -1.5693, -1.5154, -1.5279, -1.5154, -1.0663, -1.2095,\n",
       "          -1.4048, -1.4127, -1.4386, -1.2367, -1.4140, -1.4469, -1.3506,\n",
       "          -1.2763, -1.2316, -1.4160, -1.5214, -1.4621, -1.0346, -1.3469,\n",
       "          -0.8448, -1.6379, -1.3662, -1.4092, -1.5021, -1.4122, -1.3670,\n",
       "          -1.2678, -1.7959, -1.6370, -1.4791, -1.4486, -1.1611, -1.0614,\n",
       "          -1.4251, -1.1757, -1.3276, -1.0438, -1.6692, -0.9827, -1.0997,\n",
       "          -1.3646, -1.3404, -1.1344, -1.2726, -1.5097, -1.3592, -1.0338,\n",
       "          -1.4132, -0.9814, -1.2697, -1.2283, -1.2936, -1.2072, -1.5248,\n",
       "          -1.3773, -1.4261, -1.5404, -0.9925, -1.5549, -0.5699, -1.3257,\n",
       "          -1.4726, -1.1975],\n",
       "         [ 1.1688,  1.4312,  1.3459,  1.3342,  1.3827,  1.1310,  1.3739,\n",
       "           1.5694,  1.3430,  1.6227,  1.1020,  1.3528,  1.3231,  1.3009,\n",
       "           1.3403,  0.8263,  1.5200,  1.4152,  0.8949,  1.2779,  0.9424,\n",
       "           0.7056,  0.6699,  1.2051,  1.2610,  1.5562,  1.3494,  0.8702,\n",
       "           1.5857,  0.8338,  1.4981,  1.6073,  1.1067,  1.3058,  1.3133,\n",
       "           0.8537,  1.5693,  1.5154,  1.5279,  1.5154,  1.0663,  1.2095,\n",
       "           1.4048,  1.4127,  1.4386,  1.2367,  1.4140,  1.4469,  1.3506,\n",
       "           1.2763,  1.2316,  1.4160,  1.5214,  1.4621,  1.0346,  1.3469,\n",
       "           0.8448,  1.6386,  1.3662,  1.4092,  1.5021,  1.4122,  1.3670,\n",
       "           1.2678,  1.7959,  1.6370,  1.4791,  1.4486,  1.1611,  1.0614,\n",
       "           1.4251,  1.1757,  1.3276,  1.0438,  1.6692,  0.9827,  1.0997,\n",
       "           1.3646,  1.3404,  1.1344,  1.2726,  1.5097,  1.3592,  1.0338,\n",
       "           1.4132,  0.9814,  1.2697,  1.2283,  1.2936,  1.2072,  1.5248,\n",
       "           1.3773,  1.4261,  1.5404,  0.9925,  1.5549,  0.5699,  1.3257,\n",
       "           1.4726,  1.1975]]], device='cuda:0')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_probs*scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5bd9336e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3, 19, 19,  1, 11,  8,  8,  2,  4, 26, 27, 17,  7, 12, 21,  7,  9,\n",
       "           7, 15,  1, 25, 13, 25,  3, 13, 28, 25, 20, 24,  2,  8, 23, 26,  1,\n",
       "          22,  6, 22,  5, 12,  8, 10, 12, 24, 22, 28,  8, 17, 27,  6, 10,  6,\n",
       "          23, 25, 18, 22,  9,  0,  4, 10, 26,  4, 20,  5, 21, 17, 25, 19,  0,\n",
       "          11, 22, 10, 11, 15,  1, 25, 17,  9, 25, 23, 10, 22, 15,  3,  5,  1,\n",
       "          28, 12, 16, 28, 22, 22, 14, 18, 26, 17, 27,  1, 24, 19, 17],\n",
       "         [ 3, 19, 19,  1, 11,  8,  8,  2,  4, 26, 27, 17,  7, 12, 21,  7,  9,\n",
       "           7, 15,  1, 16, 13, 25,  3, 13, 28, 25, 20, 24,  2,  8, 23, 26,  1,\n",
       "          22,  6, 22,  5, 12,  8, 10, 12, 24, 22, 28,  8, 17, 27,  6, 10,  6,\n",
       "          23, 25, 18, 22,  9,  0,  4, 10, 26,  4, 20,  5, 21, 17, 25, 19,  0,\n",
       "          11, 22, 10, 11, 15,  1, 25, 17,  9, 25, 23, 10, 22, 15,  3,  5,  1,\n",
       "          28, 12, 16, 28, 22, 22, 14, 18, 26, 17, 27,  1, 24, 19, 17]],\n",
       "\n",
       "        [[21, 25, 13, 12,  9, 10,  6,  0, 23, 14,  5, 18,  9, 19, 10, 22, 14,\n",
       "          26,  2,  7,  3, 15,  5, 21,  5, 10, 25, 19, 18,  2, 22,  7, 25, 25,\n",
       "          18,  0,  1, 15,  7, 13,  3, 15,  5,  3, 17, 10, 24, 22, 15,  1,  4,\n",
       "           1, 23, 17,  5,  0, 12,  0, 18, 25, 15,  3, 10, 14, 26, 17, 20,  2,\n",
       "           2, 20,  1,  9,  3, 17,  8,  7,  2, 15, 27, 12, 26, 25, 15, 26, 10,\n",
       "          12, 16, 14, 22, 19,  3,  5,  8,  4, 14, 17,  9,  7, 12, 25],\n",
       "         [21, 25, 13, 12,  9, 10,  6,  0, 23, 14,  5, 18,  9, 19, 10, 22, 14,\n",
       "          26,  2,  7,  3, 15,  5, 21,  5, 10, 25, 19, 18,  2, 22,  7, 25, 25,\n",
       "          18,  0,  1, 15,  7, 13,  3, 15,  5,  3, 17, 10, 24, 22, 15,  1,  4,\n",
       "           1, 23, 17,  5,  0, 12,  5, 18, 25, 15,  3, 10, 14, 26, 17, 20,  2,\n",
       "           2, 20,  1,  9,  3, 17,  8,  7,  2, 15, 27, 12, 26, 25, 15, 26, 10,\n",
       "          12, 16, 14, 22, 19,  3,  5,  8,  4, 14, 17,  9,  7, 12, 25]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences # bsz,beam,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed129f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "22f4cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using PyTorch\n",
    "import torch\n",
    "\n",
    "vocab = [' ', \"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '?']\n",
    "len(vocab) # 29-1 is the shape \n",
    "\n",
    "idx2char = {i:c for i,c in enumerate(vocab)}\n",
    "\n",
    "def ctc_merge_string(s: str, blank_char='?'):\n",
    "    merged = []\n",
    "    prev = None\n",
    "    for c in s:\n",
    "        if c == blank_char:\n",
    "            prev = None  # reset repetition check on blank\n",
    "            continue\n",
    "        if c != prev:\n",
    "            merged.append(c)\n",
    "        prev = c\n",
    "    return ''.join(merged)\n",
    "   \n",
    "\n",
    "# decode_seq(sequences, vocab)[0]\n",
    "# sequences.shape torch.Size([128, 2, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9ee00fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"BRR'JGGACYZPFKTFHFN'XLXBL?XSWAGVY'UEUDKGIKWU?GPZEIEVXQUH CIYCSDTPXR JUIJN'XPHXVIUNBD'?KO?UUMQYPZ'WRP\", \"BRR'JGGACYZPFKTFHFN'OLXBL?XSWAGVY'UEUDKGIKWU?GPZEIEVXQUH CIYCSDTPXR JUIJN'XPHXVIUNBD'?KO?UUMQYPZ'WRP\"], [\"TXLKHIE VMDQHRIUMYAFBNDTDIXRQAUFXXQ 'NFLBNDBPIWUN'C'VPD K QXNBIMYPSAAS'HBPGFANZKYXNYIKOMURBDGCMPHFKX\", \"TXLKHIE VMDQHRIUMYAFBNDTDIXRQAUFXXQ 'NFLBNDBPIWUN'C'VPD KDQXNBIMYPSAAS'HBPGFANZKYXNYIKOMURBDGCMPHFKX\"]]\n"
     ]
    }
   ],
   "source": [
    "# 1) move to CPU & to plain Python list of lists\n",
    "sentences = []\n",
    "for b in range(sequences.shape[0]):\n",
    "    rows = sequences[b].cpu().tolist() # beam,T\n",
    "    decoded_beams = [''.join(idx2char[i] for i in row) for row in rows]\n",
    "    sentences.append(decoded_beams)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "37bcfa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR'JGACYZPFKTFHFN'XLXBLXSWAGVY'UEUDKGIKWUGPZEIEVXQUH CIYCSDTPXR JUIJN'XPHXVIUNBD'KOUMQYPZ'WRP\n",
      "BR'JGACYZPFKTFHFN'OLXBLXSWAGVY'UEUDKGIKWUGPZEIEVXQUH CIYCSDTPXR JUIJN'XPHXVIUNBD'KOUMQYPZ'WRP\n",
      "TXLKHIE VMDQHRIUMYAFBNDTDIXRQAUFXQ 'NFLBNDBPIWUN'C'VPD K QXNBIMYPSAS'HBPGFANZKYXNYIKOMURBDGCMPHFKX\n",
      "TXLKHIE VMDQHRIUMYAFBNDTDIXRQAUFXQ 'NFLBNDBPIWUN'C'VPD KDQXNBIMYPSAS'HBPGFANZKYXNYIKOMURBDGCMPHFKX\n"
     ]
    }
   ],
   "source": [
    "for i in sentences:\n",
    "    for j in i:\n",
    "        print(ctc_merge_string(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d94a786c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 100])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22d713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a152fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            beam_search        18.14%     695.342us        99.55%       3.817ms       3.817ms     761.000us        19.91%       3.823ms       3.823ms             1  \n",
      "                                     aten::floor_divide         5.20%     199.467us        15.51%     594.651us      33.036us     315.000us         8.24%     660.000us      36.667us            18  \n",
      "                                             aten::topk         6.33%     242.816us        13.55%     519.406us      51.941us     474.000us        12.40%     583.000us      58.300us            10  \n",
      "                                        aten::remainder         4.10%     157.227us         9.15%     350.742us      19.486us     266.000us         6.96%     422.000us      23.444us            18  \n",
      "                                           aten::gather         5.72%     219.355us         9.09%     348.351us      38.706us     314.000us         8.21%     386.000us      42.889us             9  \n",
      "                                            aten::slice         4.33%     165.892us         7.10%     272.096us      13.605us     256.000us         6.70%     345.000us      17.250us            20  \n",
      "                                       aten::as_strided         1.60%      61.279us         1.60%      61.279us       0.806us     316.000us         8.27%     316.000us       4.158us            76  \n",
      "                                        aten::unsqueeze         3.32%     127.284us         5.85%     224.197us      11.800us     215.000us         5.62%     292.000us      15.368us            19  \n",
      "                                              aten::cat         3.54%     135.677us         4.55%     174.301us      19.367us     213.000us         5.57%     213.000us      23.667us             9  \n",
      "                          fused_unsqueeze_unsqueeze_add         2.82%     107.994us         3.82%     146.287us      16.254us     186.000us         4.87%     186.000us      20.667us             9  \n",
      "                                           aten::select         2.42%      92.730us         3.76%     144.314us      14.431us     140.000us         3.66%     181.000us      18.100us            10  \n",
      "                                           aten::expand         2.03%      77.801us         3.22%     123.636us      13.737us     121.000us         3.17%     158.000us      17.556us             9  \n",
      "                                       aten::contiguous         0.18%       7.023us         2.70%     103.386us     103.386us      11.000us         0.29%     109.000us     109.000us             1  \n",
      "                                            aten::clone         0.34%      13.207us         2.40%      92.025us      92.025us      19.000us         0.50%      98.000us      98.000us             1  \n",
      "                                            aten::empty         1.33%      50.849us         1.33%      50.849us       5.085us      87.000us         2.28%      87.000us       8.700us            10  \n",
      "                                             aten::view         0.72%      27.721us         0.72%      27.721us       3.080us      61.000us         1.60%      61.000us       6.778us             9  \n",
      "                                            aten::copy_         0.86%      33.122us         1.28%      48.952us      48.952us      55.000us         1.44%      55.000us      55.000us             1  \n",
      "                                       aten::empty_like         0.23%       8.686us         0.54%      20.559us      20.559us      13.000us         0.34%      24.000us      24.000us             1  \n",
      "                                        cudaEventRecord        26.92%       1.032ms        26.92%       1.032ms       2.234us       0.000us         0.00%       0.000us       0.000us           462  \n",
      "                                       cudaLaunchKernel         6.42%     246.179us         6.42%     246.179us       4.319us       0.000us         0.00%       0.000us       0.000us            57  \n",
      "                                 cudaDeviceGetAttribute         0.37%      13.996us         0.37%      13.996us       0.350us       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                  cudaFuncGetAttributes         0.88%      33.672us         0.88%      33.672us       3.367us       0.000us         0.00%       0.000us       0.000us            10  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         1.01%      38.882us         1.01%      38.882us       0.486us       0.000us         0.00%       0.000us       0.000us            80  \n",
      "                                         cuLaunchKernel         1.00%      38.293us         1.00%      38.293us       4.255us       0.000us         0.00%       0.000us       0.000us             9  \n",
      "                                  cudaDeviceSynchronize         0.19%       7.264us         0.19%       7.264us       7.264us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.834ms\n",
      "Self CUDA time total: 3.823ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_device = 'cuda') as prof:\n",
    "    sequences, scores = beam_search(log_probs, beam_size)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcac3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22aa42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
