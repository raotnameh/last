checkpoint: 
  step: 1000

logging: 
  dir: "/raid/home/rajivratn/hemant_rajivratn/grpo_ctc/src/logs"
  step: 100

device: "cuda"

dataset_speech:
  train_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/train_1h.tsv"
  # train_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/train.tsv"
  val_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/val.tsv"
  test_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/test.tsv"

  min_duration: 32000
  max_duration: 320000
  batch_size: &batch_size_value 8

dataset_txt:
  path: "/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd"
  batch_size: *batch_size_value
  skip_non_speech: True

train:
  seed: 42123
  steps: 20000
  accumulation_steps: 1
  grad_clip: 1.0
  checkpoint_path: False #"/raid/home/rajivratn/hemant_rajivratn/grpo/src/logs_dec/checkpoints/step_052603.pt"

  groups: 8 # this is the beam size.

  ctc: True
  
  freeze_steps: 15000
  lr: 0.0001 # encoder learning rate
  temp: 10.0

  decoder: False
  decoder_grad_scale: 0.00
  dlr: 0.0005
  
  teacher: False
  beta: 0.01
  
lr_scheduler:
  phase_ratio: [0.1, 0.0, 0.9]

codebook:
  # model_name: "meta-llama/Llama-3.2-1B"
  model_name: "gpt2"

encoder:
  ckpt_path: "/raid/home/rajivratn/hemant_rajivratn/last/weights/hubert_base_ls960.pt"
  # ckpt_path: "/raid/home/rajivratn/hemant_rajivratn/last/weights/convert_iter3.pt"
  encoder_embed_dim: 768
  frozen_layers: ["11"]

decoder:
  speaker:
    use_s : False
    speaker_emb_dim: 512
    
  transformer:
    decoder_layer: 4
    decoder_head: 2
    decoder_hidden: 256
    conv_filter_size: 1024
    conv_kernel_size: [9, 1]
    decoder_dropout: 0.2
    dac_hidden: 80
  max_seq_len: 2048
