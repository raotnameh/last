checkpoint: 
  dir: "checkpoints"
  step: 5000

logging: 
  dir: "/raid/home/rajivratn/hemant_rajivratn/last/src/logs"
  step: 100

device: "cuda"

dataset_speech:
  # train_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/train.tsv"
  # val_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/val.tsv"
  # test_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/test.tsv"

  train_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/ljspeech/manifest/train.tsv"
  val_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/ljspeech/manifest/val.tsv"
  test_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/ljspeech/manifest/test.tsv"

  min_duration: 32000
  max_duration: 320000
  batch_size: &batch_size_value 50

dataset_txt:
  path: "/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd"
  # path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech-lm-norm.txt"
  batch_size: *batch_size_value

codebook:
  model_name: "meta-llama/Llama-3.2-1B-Instruct"

encoder:
  # ckpt_path: "../weights/hubert_base_ls960.pt"
  ckpt_path: "../weights/convert_iter3.pt"
  encoder_embed_dim: 768
  frozen_layers: ["11"]

downsample:
  kernel_size: &kernel_size_value 11
  stride: &stride_value 2
  groups: &groups_value 64

upsample:
  kernel_size: *kernel_size_value
  stride: *stride_value
  groups: *groups_value 

discriminator:
  hidden_dim: 256
  num_layers: 8
  kernel_size: 11

decoder:
  speaker:
    use_s : False
    speaker_emb_dim: 512
    
  transformer:
    decoder_layer: 8
    decoder_head: 2
    decoder_hidden: 256
    conv_filter_size: 1024
    conv_kernel_size: [9, 1]
    decoder_dropout: 0.2
    dac_hidden: 1024
  max_seq_len: 2048

train:
  seed: 42123
  train_vqvae: False
  train_disc: True
  train_full: False

  lr_enc: 0.0001
  lr_down: 0.0001
  lr_dec: 0.0001
  lr_disc: 0.0001
  grad_clip: 10.0
  
  num_steps: 50000
  freeze_steps: 1000
  gradient_accumulation_steps: 1
  discriminator_freq: 2 # every N steps

  mixed_precision: False
  resume_checkpoint: False
  resume_path: "checkpoints/step_10000.pth"

loss:
  recon_loss_weight: 1.0
  smooth_loss_weight: 1.0
  commit_loss_weight: 1.0
  gen_loss_weight: 1.0
  
  disc_loss_weight: 1.0
  gp_weight: 1.0

lr_scheduler:
  phase_ratio: [0.05, 0.05, 0.9]

eval:
  eval: False