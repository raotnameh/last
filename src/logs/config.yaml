checkpoint:
  step: 10000
codebook:
  model_name: meta-llama/Llama-3.2-1B
dataset_speech:
  batch_size: 32
  max_duration: 160000
  min_duration: 32000
  test_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/test.tsv
  train_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/train.tsv
  val_path: /raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/val.tsv
dataset_txt:
  batch_size: 32
  path: /raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd
  skip_non_speech: true
device: cuda
downsample:
  groups: 64
  kernel_size: 11
  stride: 1
encoder:
  ckpt_path: /raid/home/rajivratn/hemant_rajivratn/last/weights/hubert_base_ls960.pt
  encoder_embed_dim: 768
  frozen_layers:
  - '11'
logging:
  dir: /raid/home/rajivratn/hemant_rajivratn/grpo/src/logs
  step: 10
lr_scheduler:
  phase_ratio:
  - 0.1
  - 0.0
  - 0.9
train:
  accumulation_steps: 1
  epochs: 100
  epsilon: 0.2
  freeze_epochs: 1
  grad_clip: 1.0
  groups: 512
  iters: 2
  lr: 5.0e-05
  resume_checkpoint: false
  resume_path: checkpoints/step_10000.pth
  seed: 42123
  temp: 1.0
