{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speakers: 585\n",
      "Total samples: 24473, Total speakers: 585\n"
     ]
    }
   ],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        input_manifest = \"/raid/home/rajivratn/hemant_rajivratn/librispeech/data/manifest/train-clean-100.tsv\"\n",
    "\n",
    "        # Read the first line to get the root directory\n",
    "        with open(input_manifest, \"r\") as infile:\n",
    "            root_dir = infile.readline().strip()  # First line is the root directory\n",
    "\n",
    "        # Define valid duration range\n",
    "        min_duration = 32000  # 2 seconds\n",
    "        max_duration = 250000  # 15.625 seconds\n",
    "\n",
    "        # Dictionary to store filtered samples per speaker\n",
    "        filtered_samples_by_speaker = {}\n",
    "\n",
    "        with open(input_manifest, \"r\") as infile:\n",
    "            infile.readline()  # Skip header (already read root_dir)\n",
    "            for line in infile:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                file_name, duration = parts\n",
    "                duration = int(duration)\n",
    "\n",
    "                if min_duration <= duration <= max_duration:\n",
    "                    full_path = os.path.join(root_dir, file_name)\n",
    "                    speaker_id = file_name.split(\"_\")[1]  # Extract speaker ID\n",
    "                    \n",
    "                    if speaker_id not in filtered_samples_by_speaker:\n",
    "                        filtered_samples_by_speaker[speaker_id] = []\n",
    "                    \n",
    "                    filtered_samples_by_speaker[speaker_id].append((full_path, duration))\n",
    "\n",
    "        self.diff_speakers = len(filtered_samples_by_speaker)\n",
    "        print(f\"Total speakers: {self.diff_speakers}\")\n",
    "        # a tuple with path, speaker, duration\n",
    "        filtered_samples = []\n",
    "        count = 0\n",
    "        for k in filtered_samples_by_speaker:\n",
    "            count += 1\n",
    "            for i in filtered_samples_by_speaker[k]:\n",
    "                filtered_samples.append((i[0], count, i[1]))\n",
    "            #     break\n",
    "            # if len(filtered_samples) == 40: \n",
    "            # break\n",
    "            \n",
    "        print(f\"Total samples: {len(filtered_samples)}, Total speakers: {count}\")\n",
    "        # Sort by duration\n",
    "        filtered_samples.sort(key=lambda x: x[-1])\n",
    "\n",
    "        self.dataset = filtered_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, speaker, duration = self.dataset[idx]\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        assert sample_rate == 16000, \"Sampling rate must be 16000\"\n",
    "        return waveform.squeeze(0), speaker, duration\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = AudioDataset()\n",
    "\n",
    "# create a collate function to truncate the audio files to minimum length\n",
    "def collate_fn(batches):\n",
    "    min_dur = min([batch[2] for batch in batches])\n",
    "    waveforms = []\n",
    "    speakers = []\n",
    "    for batch in batches:\n",
    "        waveforms.append(batch[0][:min_dur])\n",
    "        speakers.append(batch[1])\n",
    "    return torch.stack(waveforms), torch.tensor(speakers).unsqueeze(1) # bsz, seq_len and bsz, 1\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/rajivratn/anaconda3/envs/last/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/home/rajivratn/anaconda3/envs/last/lib/python3.10/site-packages/torch/utils/_contextlib.py:125: UserWarning: Decorating classes is deprecated and will be disabled in future versions. You should only decorate functions or methods. To preserve the current behavior of class decoration, you can directly decorate the `__init__` method and nothing else.\n",
      "  warnings.warn(\"Decorating classes is deprecated and will be disabled in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports are successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/rajivratn/anaconda3/envs/last/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/raid/home/rajivratn/anaconda3/envs/last/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: ' ', 2: \"'\", 3: 'A', 4: 'B', 5: 'C', 6: 'D', 7: 'E', 8: 'F', 9: 'G', 10: 'H', 11: 'I', 12: 'J', 13: 'K', 14: 'L', 15: 'M', 16: 'N', 17: 'O', 18: 'P', 19: 'Q', 20: 'R', 21: 'S', 22: 'T', 23: 'U', 24: 'V', 25: 'W', 26: 'X', 27: 'Y', 28: 'Z', 29: '<sil>', 30: '<bos>', 31: '<eos>'}\n",
      "Paraeters of spk_embed: 0.14976\n",
      "Paraeters of downsampling: 4.130304\n",
      "Paraeters of upsampling: 0.786688\n",
      "Paraeters of decoder: 7.478272\n",
      "Models are initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import models\n",
    "from encoder import Encoder, Downsampling\n",
    "from vocab import FrozenVocabulary, get_closest_vocab, merge_similar_indices\n",
    "from decoder import Upsampling, Decoder, calculate_params\n",
    "from codec import Codec\n",
    "\n",
    "print(\"All imports are successful\")\n",
    "\n",
    "class Spk_Embed(nn.Module):\n",
    "    def __init__(self, num_speakers=100, spk_embed_dim=256):\n",
    "        super(Spk_Embed, self).__init__()\n",
    "        self.spk_embed = nn.Embedding(num_speakers, spk_embed_dim)\n",
    "        \n",
    "    def forward(self, speaker):\n",
    "        return self.spk_embed(speaker)\n",
    "    \n",
    "# params\n",
    "hidden_dim = 256\n",
    "spk_embed_dim = 256\n",
    "num_speakers = dataset.diff_speakers\n",
    "\n",
    "# models \n",
    "spk_embed = Spk_Embed(num_speakers=num_speakers, spk_embed_dim=spk_embed_dim)\n",
    "encoder = Encoder() # frozen\n",
    "downsampling = Downsampling()\n",
    "vocab = FrozenVocabulary(path=\"vocab.pth\") # frozen\n",
    "upsampling = Upsampling(inp_dim=int(768+spk_embed_dim), hidden_dim=hidden_dim)\n",
    "decoder = Decoder(hidden_dim=hidden_dim, out_dim=1024, num_blocks=5, kernel_size=11)\n",
    "codec = Codec() # frozen\n",
    "vocab_embeddings, char_to_idx, idx_to_char = vocab.embeddings, vocab.char_to_idx, vocab.idx_to_char\n",
    "\n",
    "print(idx_to_char)\n",
    "print(f\"Paraeters of spk_embed: {calculate_params(spk_embed)}\")\n",
    "print(f\"Paraeters of downsampling: {calculate_params(downsampling)}\")\n",
    "print(f\"Paraeters of upsampling: {calculate_params(upsampling)}\")\n",
    "print(f\"Paraeters of decoder: {calculate_params(decoder)}\")\n",
    "\n",
    "print(\"Models are initialized\")\n",
    "\n",
    "\n",
    "# Set the models to gpu\n",
    "device = torch.device(\"cuda\")\n",
    "encoder = encoder.to(device)\n",
    "downsampling = downsampling.to(device)\n",
    "vocab_embeddings = vocab_embeddings.to(device)\n",
    "decoder = decoder.to(device)\n",
    "upsampling = upsampling.to(device)\n",
    "codec.model = codec.model.to(device)\n",
    "spk_embed = spk_embed.to(device)\n",
    "\n",
    "# freeze the encoder, and codec\n",
    "for param in codec.model.parameters():\n",
    "    param.requires_grad = False   \n",
    "vocab_embeddings.requires_grad = False\n",
    "\n",
    "# Training loop\n",
    "downsampling.train()\n",
    "decoder.train()\n",
    "upsampling.train()\n",
    "codec.model.eval()\n",
    "spk_embed.train()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the models to training mode\n",
    "encoder.train()\n",
    "for param in encoder.named_parameters():\n",
    "    param[1].requires_grad = False\n",
    "    continue\n",
    "    if \"model.encoder.layers.8\" in param[0] or \"model.encoder.layers.11\" in param[0]:\n",
    "        param[1].requires_grad = True\n",
    "    else:\n",
    "        param[1].requires_grad = False\n",
    "        \n",
    "optimizer = optim.Adam(\n",
    "    list(downsampling.parameters()) + list(decoder.parameters()) + list(upsampling.parameters()) + list(spk_embed.parameters()),\n",
    "    # list(downsampling.parameters()) + list(decoder.parameters()) + list(upsampling.parameters()) + list(encoder.parameters()) + list(spk_embed.parameters()),\n",
    "    lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 66]' is invalid for input of size 6336",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m downsampling_output \u001b[38;5;241m=\u001b[39m downsampling(encoder_output) \u001b[38;5;66;03m# torch.Size([32, 768, 172])\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Get the closest vocab embeddings\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m commitment_loss, vocab_output, indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_closest_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownsampling_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# add speaker embeddings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m speaker \u001b[38;5;241m=\u001b[39m spk_embed(speaker)\n",
      "File \u001b[0;32m/raid/home/rajivratn/hemant_rajivratn/last/utils/vocab.py:41\u001b[0m, in \u001b[0;36mget_closest_vocab\u001b[0;34m(z, e)\u001b[0m\n\u001b[1;32m     38\u001b[0m commitment_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(z, z_q\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m     39\u001b[0m z_q \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m (z_q \u001b[38;5;241m-\u001b[39m z)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m commitment_loss, z_q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[43mmin_encoding_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 66]' is invalid for input of size 6336"
     ]
    }
   ],
   "source": [
    "def merge_tensors(t1, t2):\n",
    "    t2 = t2.unsqueeze(-1)  # Reshape to (batch, features, 1)\n",
    "    return torch.cat([t1, t2.expand(-1, -1, t1.shape[-1])], dim=1)\n",
    "\n",
    "# start training\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    running_loss = 0.0\n",
    "    for iteration, data in enumerate(dataloader):\n",
    "        # data\n",
    "        waveform, speaker = data\n",
    "        waveform = waveform.to(device) \n",
    "        speaker = torch.tensor(speaker).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            encoder_output = encoder(waveform)\n",
    "        downsampling_output = downsampling(encoder_output) # torch.Size([32, 768, 172])\n",
    "        # Get the closest vocab embeddings\n",
    "        commitment_loss, vocab_output, indices = get_closest_vocab(downsampling_output, vocab_embeddings)\n",
    "        \n",
    "        # add speaker embeddings\n",
    "        speaker = spk_embed(speaker)\n",
    "        vocab_output = merge_tensors(vocab_output, speaker)\n",
    "\n",
    "        \n",
    "        # Upsampling\n",
    "        upsampling_output = upsampling(vocab_output)\n",
    "        # Decoder\n",
    "        decoder_output = decoder(upsampling_output).contiguous() # torch.Size([32, 1024, 172])\n",
    "        \n",
    "        # Codec\n",
    "        with torch.no_grad():\n",
    "            codec_output = codec.encode(waveform).detach().contiguous()\n",
    "        \n",
    "        # Ensure same sequence length for ground truth and output\n",
    "        min_seq_len = min(codec_output.shape[-1], decoder_output.shape[-1])    \n",
    "        codec_output = codec_output[:, :, :min_seq_len]\n",
    "        decoder_output = decoder_output[:, :, :min_seq_len]    \n",
    "\n",
    "        # Compute the loss\n",
    "        l2_loss = F.mse_loss(decoder_output, codec_output)\n",
    "        # commitment_loss *= 10\n",
    "        loss =  l2_loss + commitment_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # empty cache\n",
    "        torch.cuda.empty_cache()  \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # print for every 10 iterations\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Indices: {indices[0]}\")\n",
    "            print(f\"Epoch: {epoch}, Iteration: {iteration}/{len(dataloader)}, Loss: {running_loss/(iteration+1)}, commit_loss: {commitment_loss.item()}, l2_loss: {l2_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 768, 66]), torch.Size([29, 256]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampling_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = indices\n",
    "\"\".join([idx_to_char[i] for i in merge_similar_indices(ind)[0]]) #.replace(\"<sil>\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode audio signal\n",
    "y = codec.model.decode(decoder_output[1:2,:,:]).cpu().detach().numpy()\n",
    "\n",
    "# play the numpy array as audio using ipython.display.Audio\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(y[0,0,:], rate=16000)  # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(waveform[1,:].cpu().detach().numpy(), rate=16000)  # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
