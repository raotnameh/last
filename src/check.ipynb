{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEMANT IS MY NAME',\n",
       " 'HEEEMMMANNNNT IIIIS MMMMY N NAAAME EE',\n",
       " 'DRNOT TL KTCRNT ST RCT S C CDR C T DGT NE']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [\n",
    "    \"Hemant is my name\",\n",
    "    \"Heeemmmannnnt iiiis mmmmy n naaame ee\",\n",
    "    \"DRNOT TL KTCRNT ST RCT S C CDR C T DGT NE\"\n",
    "]\n",
    "\n",
    "input_texts = [\"\".join(i.upper()) for i in input_texts ]\n",
    "input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/rajivratn/anaconda3/envs/langspeech/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/home/rajivratn/anaconda3/envs/langspeech/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(375, 1536, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-28): 29 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (k_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (v_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=375, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import charactertokenizer\n",
    "\n",
    "# model_dir = 'ai-forever/charllama-2.6B'\n",
    "model_dir = 'ai-forever/charllama-1.3B'\n",
    "\n",
    "tokenizer = charactertokenizer.CharacterTokenizer.from_pretrained(model_dir)\n",
    "# print(tokenizer.get_vocab().keys())\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# save_dir = './charllama_tokenizer'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Save tokenizer files\n",
    "# tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# # Load tokenizer config (contains char_ords)\n",
    "# config_path = os.path.join(save_dir, 'tokenizer_config.json')\n",
    "# with open(config_path, 'r', encoding='utf-8') as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "# # Extract char ordinals\n",
    "# char_ords = config.get(\"char_ords\", [])\n",
    "\n",
    "# # Convert to readable characters\n",
    "# char_list = []\n",
    "# for c in char_ords:\n",
    "#     try:\n",
    "#         char_list.append({'ord': c, 'char': chr(c)})\n",
    "#     except:\n",
    "#         char_list.append({'ord': c, 'char': '<Invalid>'})\n",
    "\n",
    "# # Save as JSON\n",
    "# with open(os.path.join(save_dir, 'vocab_chars.json'), 'w', encoding='utf-8') as f:\n",
    "#     json.dump(char_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# # Save as plain text\n",
    "# with open(os.path.join(save_dir, 'vocab_chars.txt'), 'w', encoding='utf-8') as f:\n",
    "#     for item in char_list:\n",
    "#         f.write(f\"{item['ord']}: {item['char']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " '<sep>': 4,\n",
       " '<cls>': 5,\n",
       " '<mask>': 6,\n",
       " '\\x08': 7,\n",
       " '–ó': 8,\n",
       " '–∞': 9,\n",
       " ' ': 10,\n",
       " '–æ': 11,\n",
       " '–∫': 12,\n",
       " '–Ω': 13,\n",
       " 'ÃÅ': 14,\n",
       " '–º': 15,\n",
       " '–ø': 16,\n",
       " '—Ä': 17,\n",
       " '—Å': 18,\n",
       " '–µ': 19,\n",
       " '–ª': 20,\n",
       " '–∏': 21,\n",
       " '—è': 22,\n",
       " '–±': 23,\n",
       " '—ã': 24,\n",
       " ',': 25,\n",
       " '\\n': 26,\n",
       " '—á': 27,\n",
       " '—Ç': 28,\n",
       " '—å': 29,\n",
       " '–≥': 30,\n",
       " '.': 31,\n",
       " '–í': 32,\n",
       " '–≤': 33,\n",
       " '—ë': 34,\n",
       " '–∑': 35,\n",
       " '–ù': 36,\n",
       " '—É': 37,\n",
       " '—é': 38,\n",
       " '–¥': 39,\n",
       " '–Ø': 40,\n",
       " '—Ö': 41,\n",
       " '—à': 42,\n",
       " '–π': 43,\n",
       " '–≠': 44,\n",
       " '–ö': 45,\n",
       " '–ë': 46,\n",
       " '—â': 47,\n",
       " '–û': 48,\n",
       " '–°': 49,\n",
       " '–î': 50,\n",
       " '—ç': 51,\n",
       " '-': 52,\n",
       " '–∂': 53,\n",
       " '—Ü': 54,\n",
       " '–ò': 55,\n",
       " '–ü': 56,\n",
       " '–ß': 57,\n",
       " '–ê': 58,\n",
       " '–¢': 59,\n",
       " '\\x0c': 60,\n",
       " '\\x0b': 61,\n",
       " '–†': 62,\n",
       " '2': 63,\n",
       " '0': 64,\n",
       " '8': 65,\n",
       " '9': 66,\n",
       " '4': 67,\n",
       " '3': 68,\n",
       " '%': 69,\n",
       " '7': 70,\n",
       " '(': 71,\n",
       " '—Ñ': 72,\n",
       " ')': 73,\n",
       " '1': 74,\n",
       " '–ú': 75,\n",
       " '!': 76,\n",
       " '–¶': 77,\n",
       " '5': 78,\n",
       " '—ä': 79,\n",
       " '6': 80,\n",
       " '$': 81,\n",
       " '–õ': 82,\n",
       " ':': 83,\n",
       " '¬´': 84,\n",
       " '¬ª': 85,\n",
       " '–£': 86,\n",
       " '–•': 87,\n",
       " '?': 88,\n",
       " '–ì': 89,\n",
       " '–§': 90,\n",
       " '–ñ': 91,\n",
       " '–®': 92,\n",
       " '–ï': 93,\n",
       " '\"': 94,\n",
       " '‚Ä¶': 95,\n",
       " '–Å': 96,\n",
       " ';': 97,\n",
       " '–©': 98,\n",
       " '‚Äû': 99,\n",
       " '‚Äú': 100,\n",
       " '–Æ': 101,\n",
       " '‚Ññ': 102,\n",
       " '¬ß': 103,\n",
       " '/': 104,\n",
       " '–´': 105,\n",
       " '‚Äô': 106,\n",
       " '–ô': 107,\n",
       " '–¨': 108,\n",
       " \"'\": 109,\n",
       " ']': 110,\n",
       " '*': 111,\n",
       " '[': 112,\n",
       " '<': 113,\n",
       " '>': 114,\n",
       " '–™': 115,\n",
       " '‚Äù': 116,\n",
       " '+': 117,\n",
       " '¬±': 118,\n",
       " '=': 119,\n",
       " '‡§ï': 120,\n",
       " '‡§≤': 121,\n",
       " '‡§π': 122,\n",
       " '‡§†': 123,\n",
       " '‡§Ø': 124,\n",
       " '‡•ã': 125,\n",
       " '‡§ó': 126,\n",
       " '‡§™': 127,\n",
       " '‡•ç': 128,\n",
       " '‡§∞': 129,\n",
       " '‡§¶': 130,\n",
       " '‡•Ä': 131,\n",
       " '‡§ø': 132,\n",
       " '‡§æ': 133,\n",
       " '√©': 134,\n",
       " '√≥': 135,\n",
       " '\\\\': 136,\n",
       " '√ó': 137,\n",
       " 'ÃÜ': 138,\n",
       " '‚Ç¨': 139,\n",
       " '√∫': 140,\n",
       " '&': 141,\n",
       " '–à': 142,\n",
       " '¬¶': 143,\n",
       " '‚Äπ': 144,\n",
       " '¬∞': 145,\n",
       " '·∞¥': 146,\n",
       " '‚¨ì': 147,\n",
       " '‚Ä∫': 148,\n",
       " '—ò': 149,\n",
       " '—ô': 150,\n",
       " 'ÔøΩ': 151,\n",
       " '√Ω': 152,\n",
       " '\\x0f': 153,\n",
       " '¬¥': 154,\n",
       " '—ú': 155,\n",
       " 'T': 156,\n",
       " 'e': 157,\n",
       " 'Q': 158,\n",
       " 'u': 159,\n",
       " 'i': 160,\n",
       " 'r': 161,\n",
       " 'o': 162,\n",
       " 'c': 163,\n",
       " 'R': 164,\n",
       " '{': 165,\n",
       " '}': 166,\n",
       " '@': 167,\n",
       " 'b': 168,\n",
       " 'n': 169,\n",
       " 'P': 170,\n",
       " 's': 171,\n",
       " 'S': 172,\n",
       " 'O': 173,\n",
       " 'C': 174,\n",
       " 'G': 175,\n",
       " 'l': 176,\n",
       " 'f': 177,\n",
       " 'd': 178,\n",
       " 'm': 179,\n",
       " 'a': 180,\n",
       " 'g': 181,\n",
       " 'I': 182,\n",
       " 'v': 183,\n",
       " 'y': 184,\n",
       " 'E': 185,\n",
       " 't': 186,\n",
       " 'h': 187,\n",
       " 'D': 188,\n",
       " 'p': 189,\n",
       " '#': 190,\n",
       " 'N': 191,\n",
       " '^': 192,\n",
       " 'Z': 193,\n",
       " 'A': 194,\n",
       " 'L': 195,\n",
       " 'M': 196,\n",
       " 'B': 197,\n",
       " 'V': 198,\n",
       " 'w': 199,\n",
       " 'k': 200,\n",
       " 'Y': 201,\n",
       " 'X': 202,\n",
       " 'F': 203,\n",
       " '\\x05': 204,\n",
       " 'x': 205,\n",
       " 'H': 206,\n",
       " 'j': 207,\n",
       " 'U': 208,\n",
       " 'q': 209,\n",
       " '—ñ': 210,\n",
       " '√°': 211,\n",
       " 'K': 212,\n",
       " 'W': 213,\n",
       " '–Ü': 214,\n",
       " 'z': 215,\n",
       " '—ï': 216,\n",
       " 'J': 217,\n",
       " '¬∫': 218,\n",
       " '—û': 219,\n",
       " '–é': 220,\n",
       " '\\x1a': 221,\n",
       " 'Õ£': 222,\n",
       " 'Õ®': 223,\n",
       " '—ö': 224,\n",
       " '—õ': 225,\n",
       " '—ì': 226,\n",
       " '–å': 227,\n",
       " '–Ö': 228,\n",
       " 'ÃÄ': 229,\n",
       " '\\x10': 230,\n",
       " '‚Ä≤': 231,\n",
       " '—ù': 232,\n",
       " '‚Äê': 233,\n",
       " '¬£': 234,\n",
       " '¬º': 235,\n",
       " '\\u3000': 236,\n",
       " '¬π': 237,\n",
       " '¬≤': 238,\n",
       " '¬≥': 239,\n",
       " '–è': 240,\n",
       " '‚Äî': 241,\n",
       " '‚Äì': 242,\n",
       " '¬§': 243,\n",
       " '¬¢': 244,\n",
       " '`': 245,\n",
       " '\\u202c': 246,\n",
       " '‚Äï': 247,\n",
       " '‚àí': 248,\n",
       " '|': 249,\n",
       " '¬∂': 250,\n",
       " '‚Äö': 251,\n",
       " '\\x11': 252,\n",
       " ' º': 253,\n",
       " '\\u202d': 254,\n",
       " '¬Æ': 255,\n",
       " '¬•': 256,\n",
       " '‚î¥': 257,\n",
       " '¬∑': 258,\n",
       " '\\x98': 259,\n",
       " '\\uf00a': 260,\n",
       " 'Ãß': 261,\n",
       " '·∏ø': 262,\n",
       " 'ÀÜ': 263,\n",
       " 'Œö': 264,\n",
       " 'Œ±': 265,\n",
       " 'Œª': 266,\n",
       " 'Œ∑': 267,\n",
       " 'Œº': 268,\n",
       " 'Œµ': 269,\n",
       " 'œÅ': 270,\n",
       " 'ƒ∏': 271,\n",
       " '\\ufeff': 272,\n",
       " '‚àô': 273,\n",
       " '“õ': 274,\n",
       " '‚ñµ': 275,\n",
       " 'Íê¶': 276,\n",
       " '‡≤†': 277,\n",
       " '„Äà': 278,\n",
       " '„Äâ': 279,\n",
       " '‚Äñ': 280,\n",
       " '‰åå': 281,\n",
       " ' π': 282,\n",
       " '—í': 283,\n",
       " '‚ïê': 284,\n",
       " '‚ïõ': 285,\n",
       " '—≤': 286,\n",
       " '—≥': 287,\n",
       " '–â': 288,\n",
       " '‚Ä†': 289,\n",
       " '\\x07': 290,\n",
       " '—ü': 291,\n",
       " '‚Äí': 292,\n",
       " '‚â•': 293,\n",
       " 'ÀÆ': 294,\n",
       " '\\uf0a2': 295,\n",
       " '◊ß': 296,\n",
       " '◊®': 297,\n",
       " '◊ë': 298,\n",
       " '◊†': 299,\n",
       " '\\uf0bc': 300,\n",
       " '‚ó¶': 301,\n",
       " '\\x18': 302,\n",
       " '—ó': 303,\n",
       " '‚ïï': 304,\n",
       " '‚ï¢': 305,\n",
       " '‚ï§': 306,\n",
       " '‚àö': 307,\n",
       " '‚îò': 308,\n",
       " '‚ïì': 309,\n",
       " '‚å†': 310,\n",
       " '‚ñ†': 311,\n",
       " '‚ï•': 312,\n",
       " '‚ïñ': 313,\n",
       " '‚ñå': 314,\n",
       " 'œå': 315,\n",
       " '√†': 316,\n",
       " '‚Ä≥': 317,\n",
       " '‚ñí': 318,\n",
       " '‚ñì': 319,\n",
       " '‚ï¨': 320,\n",
       " 'Ÿé': 321,\n",
       " '¬∏': 322,\n",
       " '√´': 323,\n",
       " 'ü§¨': 324,\n",
       " 'ƒó': 325,\n",
       " '”ô': 326,\n",
       " '“£': 327,\n",
       " '”©': 328,\n",
       " '“Ø': 329,\n",
       " '‚òº': 330,\n",
       " '‚â™': 331,\n",
       " '‚â´': 332,\n",
       " '‚†Ä': 333,\n",
       " '‚ùÑ': 334,\n",
       " 'Ô∏è': 335,\n",
       " '€•': 336,\n",
       " '√®': 337,\n",
       " 'Œú': 338,\n",
       " 'Œ∫': 339,\n",
       " 'Œ¥': 340,\n",
       " 'Œø': 341,\n",
       " 'ŒΩ': 342,\n",
       " 'ŒØ': 343,\n",
       " '√≤': 344,\n",
       " '„Ää': 345,\n",
       " '„Äã': 346,\n",
       " 'Ã∂': 347,\n",
       " 'ÔºÅ': 348,\n",
       " 'Ãà': 349,\n",
       " 'ÕÅ': 350,\n",
       " '‚úç': 351,\n",
       " '√ì': 352,\n",
       " 'Ôºà': 353,\n",
       " 'Ôºâ': 354,\n",
       " '≈∑': 355,\n",
       " '“°': 356,\n",
       " '“ô': 357,\n",
       " '“ì': 358,\n",
       " 'Àó': 359,\n",
       " 'Œ¨': 360,\n",
       " '—ø': 361,\n",
       " '\\x8d': 362,\n",
       " '\\uf008': 363,\n",
       " '·ª≥': 364,\n",
       " '\\ue02d': 365,\n",
       " 'Œß': 366,\n",
       " 'ÿò': 367,\n",
       " '‚ù§': 368,\n",
       " '“ª': 369,\n",
       " '“´': 370,\n",
       " '”ò': 371,\n",
       " 'Ëã¶': 372,\n",
       " 'ÁÑ°': 373,\n",
       " 'œÄ': 374}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEMANT IS MY NAME\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 3.6717\n",
      "Perplexity: 39.3203\n",
      "\n",
      "HEEEMMMANNNNT IIIIS MMMMY N NAAAME EE\n",
      "Cross-entropy loss: 3.8919\n",
      "Perplexity: 49.0039\n",
      "\n",
      "DRNOT TL KTCRNT ST RCT S C CDR C T DGT NE\n",
      "Cross-entropy loss: 3.4540\n",
      "Perplexity: 31.6276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for text in input_texts:\n",
    "        # Poetry completion\n",
    "        prompt =  text \n",
    "        print(prompt)\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        input_ids = inputs['input_ids']\n",
    "\n",
    "        # # Decode input IDs back to string\n",
    "        # decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "        # print(\"Decoded text from input_ids:\", repr(decoded_text))\n",
    "\n",
    "        # # Optional: Show individual tokens\n",
    "        # tokens = [tokenizer.decode([tid.item()]) for tid in input_ids[0]]\n",
    "        # print(\"\\nIndividual decoded tokens:\")\n",
    "        # for i, token in enumerate(tokens):\n",
    "        #     print(f\"Token {i}: {repr(token)} (id={input_ids[0][i].item()})\")\n",
    "            \n",
    "        # Forward pass to get logits\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits  # shape: (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        # Shift logits and labels for causal loss\n",
    "        # Predicted logits are for tokens [0 ... n-2], targets are [1 ... n-1]\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "        # Flatten the tensors for loss calculation\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                        shift_labels.view(-1))\n",
    "\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        print(f\"Cross-entropy loss: {loss.item():.4f}\")\n",
    "        print(f\"Perplexity: {perplexity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/281241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 281241/281241 [00:00<00:00, 1498635.22it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/rajivratn/anaconda3/envs/langspeech/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/home/rajivratn/anaconda3/envs/langspeech/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2669075/204741776.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/70263 [00:00<?, ?it/s]/tmp/ipykernel_2669075/204741776.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/3:   0%|          | 178/70263 [00:25<2:13:30,  8.75it/s, loss=2.2187, lr=0.000000, step=6]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import charactertokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "# --- Config ---\n",
    "model_dir = 'ai-forever/charllama-1.3B'\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 4\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "max_length = 256\n",
    "gradient_accumulation_steps = 32\n",
    "save_steps = 5000\n",
    "log_interval = 10  # Log every 10 iterations\n",
    "output_dir = \"charllama-finetuned\"  # Directory to save checkpoints\n",
    "log_dir = os.path.join(\"runs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# --- Dataset class ---\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sentences = self._load_and_preprocess(file_path)\n",
    "\n",
    "    def _load_and_preprocess(self, file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        return [line.strip().upper() for line in lines if len(line.strip()) > 10]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tokens = self.tokenizer(sentence,\n",
    "                                max_length=self.max_length,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_tensors='pt')\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# --- Load tokenizer ---\n",
    "tokenizer = charactertokenizer.CharacterTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# --- Load model, optimizer, scheduler, and scaler from checkpoint (if available) ---\n",
    "checkpoint_path = None  # Set to the checkpoint directory if you want to resume training\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_dir).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,  # Will be updated if loading from checkpoint\n",
    "    num_training_steps=1 # Initialize with a non-zero value to avoid the check if loading\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "global_step = 0\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    checkpoint_dirs = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d)) and \"checkpoint-step-\" in d]\n",
    "    if checkpoint_dirs:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split('-')[-1]))\n",
    "        checkpoint_path = latest_checkpoint\n",
    "        print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(checkpoint_path).to(device)\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(checkpoint_path, 'optimizer.pt')))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(checkpoint_path, 'scheduler.pt')))\n",
    "        scaler.load_state_dict(torch.load(os.path.join(checkpoint_path, 'scaler.pt')))\n",
    "        global_step = int(checkpoint_path.split('-')[-1])\n",
    "        train_dataset_temp = CharDataset(\"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd\", tokenizer, max_length)\n",
    "        start_epoch = global_step // len(DataLoader(train_dataset_temp, batch_size=batch_size))\n",
    "        print(f\"Resuming from global step: {global_step}, epoch: {start_epoch}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found. Starting training from scratch.\")\n",
    "        train_dataset_temp = CharDataset(\"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd\", tokenizer, max_length)\n",
    "        scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=len(DataLoader(train_dataset_temp, batch_size=batch_size)) // 10,\n",
    "            num_training_steps=len(DataLoader(train_dataset_temp, batch_size=batch_size)) * num_epochs // gradient_accumulation_steps\n",
    "        )\n",
    "else:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Starting training from scratch.\")\n",
    "    train_dataset_temp = CharDataset(\"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd\", tokenizer, max_length)\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=len(DataLoader(train_dataset_temp, batch_size=batch_size)) // 10,\n",
    "        num_training_steps=len(DataLoader(train_dataset_temp, batch_size=batch_size)) * num_epochs // gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "model.train()\n",
    "\n",
    "# --- Create dataset and dataloader ---\n",
    "train_file_path = \"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd\"\n",
    "dataset = CharDataset(train_file_path, tokenizer, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Training Loop ---\n",
    "model.zero_grad()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), initial=global_step % len(dataloader) if start_epoch == epoch else 0, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for step, batch in progress_bar:\n",
    "        if start_epoch == epoch and step < global_step % len(dataloader):\n",
    "            continue  # Skip steps already done in the previous run\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            # Log loss and learning rate to TensorBoard every log_interval steps\n",
    "            if global_step % log_interval == 0:\n",
    "                writer.add_scalar('loss/step', loss.item() * gradient_accumulation_steps, global_step)\n",
    "                writer.add_scalar('learning_rate', scheduler.get_last_lr()[0], global_step)\n",
    "\n",
    "            if global_step % save_steps == 0:\n",
    "                checkpoint_dir = os.path.join(output_dir, f\"checkpoint-step-{global_step}\")\n",
    "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                model.save_pretrained(checkpoint_dir)\n",
    "                tokenizer.save_pretrained(checkpoint_dir)\n",
    "                torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, 'optimizer.pt'))\n",
    "                torch.save(scheduler.state_dict(), os.path.join(checkpoint_dir, 'scheduler.pt'))\n",
    "                torch.save(scaler.state_dict(), os.path.join(checkpoint_dir, 'scaler.pt'))\n",
    "                print(f\"Checkpoint saved at step {global_step} to {checkpoint_dir}\")\n",
    "\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        progress_bar.set_postfix({\"loss\": f\"{total_loss / (step + 1):.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.6f}\", \"step\": global_step + 1})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")\n",
    "    writer.add_scalar('loss/epoch', avg_loss, epoch + 1)\n",
    "\n",
    "# --- Save the final trained model ---\n",
    "final_output_dir = os.path.join(output_dir, \"final-model\")\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "model.save_pretrained(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "print(f\"Final model saved to {final_output_dir}\")\n",
    "\n",
    "# --- Close TensorBoard writer ---\n",
    "writer.close()\n",
    "print(f\"TensorBoard logs saved to {log_dir}\")\n",
    "print(\"To view TensorBoard logs, run: `tensorboard --logdir runs` from your terminal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
