{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(['p', ' ', \"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class Codebook(torch.nn.Module):\n",
    "    def __init__(self, vocab, model_name=\"meta-llama/Llama-3.2-1B-Instruct\"):\n",
    "        super(Codebook, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.model_name = model_name\n",
    "        # Initialize the LLM discriminator and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Set EOS token as the padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        # Freeze LLM parameters\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, embed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch, seq_len] token IDs\n",
    "        Returns:\n",
    "            perplexity: scalar reward (lower is better)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # Ensure LLM stays frozen\n",
    "            outputs = self.llm(inputs_embeds=embed, labels=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits[:, :-1, :]       # [B, T-1, V]\n",
    "            target = input_ids[:, 1:]             # [B, T-1]\n",
    "            mask = attention_mask[:, 1:].float()  # [B, T-1]\n",
    "\n",
    "            # Cross-entropy loss per token\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)), \n",
    "                target.reshape(-1), \n",
    "                reduction='none'\n",
    "            ).reshape(target.shape)  # [B, T-1]\n",
    "\n",
    "            # Mask pad tokens and compute mean loss\n",
    "            masked_loss = loss * mask  # [B, T-1]\n",
    "            token_count = mask.sum(dim=1)  # [B]\n",
    "            \n",
    "            loss = masked_loss.sum(dim=1) / token_count  \n",
    "            perplexity = torch.exp(loss)    \n",
    "\n",
    "            return loss, perplexity, outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F V   D   H E F Y Y', \"W E   D O   N O T   R E M E M B E R   H O W   T H E   F O R E S T   F E L L ,   O N L Y   T H A T   F I R E   B U R N E D   C L O S E   B E H I N D .   C H I L D R E N   C R I E D ,   O T H E R S   R A N ,   A N D   S I L E N C E   G R E W   L O U D E R   I N   T H E   S M O K Y   A I R .   D E S P E R A T E   M E N   S H O U T E D   I N T O   T H E   T R E E S ,   B U T   N O   A N S W E R   C A M E .   I N   T H A T   F A D I N G   L I G H T ,   E V E R Y T H I N G   F E L T   F I N A L — L I K E   M E M O R Y   S L I P P I N G   T H R O U G H   A   D O O R   W E   C O U L D N ' T   C L O S E .\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([5.3233, 2.2565]), tensor([205.0494,   9.5500]), tensor(4.7987))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"FV D HEFYY\", \"We do not remember how the forest fell, only that fire burned close behind. Children cried, others ran, and silence grew louder in the smoky air. Desperate men shouted into the trees, but no answer came. In that fading light, everything felt final—like memory slipping through a door we couldn't close.\"]\n",
    "\n",
    "\n",
    "# function to convert a string to a white space separated string ignoreing the spaces\n",
    "def convert_to_whitespace_string(string):\n",
    "    # Remove spaces and join characters with whitespace\n",
    "    return ' '.join(string)\n",
    "\n",
    "sentence = [convert_to_whitespace_string(s.upper()) for s in sentence]\n",
    "\n",
    "print(sentence)\n",
    "# Initialize Codebook discriminator\n",
    "codebook = Codebook(vocab=None)\n",
    "tokenizer = codebook.tokenizer\n",
    "\n",
    "# Tokenize sentence to get input IDs\n",
    "tokenized = tokenizer(sentence, padding=True, return_tensors='pt', add_special_tokens=False)\n",
    "embed = codebook.llm.get_input_embeddings()(tokenized['input_ids'])\n",
    "\n",
    "codebook(**tokenized, embed=embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F V   D   H E   O R N E C M R N H T W D O T A F   T A L   F Y Y B   C I N E   O I L O N   C F   C E   E C I B E A I   D E A V I A E U S   T M S   U N E T M R   R S T I L U N   E T F Y T L E N   O T W N R   O H L G H Y B Y   T E H N E   H T F   I   I A   R H T P G K H P D S K H O   O W H L O S G K H L Y   O E A E Y P H E R   N E O O W   C O T N T F I   D   R O N E S T   O H R S   I   I R Y R U E L   O N T   Y N   A R I N R T B N   H C F   O F   D E P I P T E   T R   A C K E A B G H T M R N D O   K A O S A V',\n",
       " \"W E   D O   N O T   R E M E M B E R   H O W   T H E   F O R E S T   F E L L ,   O N L Y   T H A T   F I R E   B U R N E D   C L O S E   B E H I N D .   C H I L D R E N   C R I E D ,   O T H E R S   R A N ,   A N D   S I L E N C E   G R E W   L O U D E R   I N   T H E   S M O K Y   A I R .   D E S P E R A T E   M E N   S H O U T E D   I N T O   T H E   T R E E S ,   B U T   N O   A N S W E R   C A M E .   I N   T H A T   F A D I N G   L I G H T ,   E V E R Y T H I N G   F E L T   F I N A L — L I K E   M E M O R Y   S L I P P I N G   T H R O U G H   A   D O O R   W E   C O U L D N ' T   C L O S E .\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 9, 9, 9, 5, 6, 7, 8, 9],\n",
       "        [1, 2, 3, 4, 5, 6, 7, 9, 9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "target = torch.tensor([\n",
    "    [1, 9, 9, 9, 5, 6, 7, 8, 9],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 9, 9]\n",
    "], dtype=torch.long)\n",
    "\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False, False]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask based on the valid indices\n",
    "valid_mask = target != 9\n",
    "\n",
    "valid_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 5, 6, 7, 8]), tensor([1, 2, 3, 4, 5, 6, 7])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = [target[i][valid_mask[i]] for i in range(target.size(0))]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 5, 6, 7, 8, 0, 0],\n",
       "        [1, 2, 3, 4, 5, 6, 7]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Pad the sequences for each tensor\n",
    "target = pad_sequence(target, batch_first=True, padding_value=0)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm.auto import tqdm  \n",
    "import logging\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Dataset_txt(Dataset):\n",
    "    def __init__(self, data=\"/raid/home/rajivratn/hemant_rajivratn/last/data/transcription.txt\"):\n",
    "        super(Dataset_txt, self).__init__()\n",
    "\n",
    "        \n",
    "        with open(data, \"r\") as f:\n",
    "            out = f.readlines()\n",
    "        texts = [x.strip() for x in tqdm(out) if len(x) > 10] # filtering out short texts that 2 second.\n",
    "    \n",
    "   \n",
    "        self.save_histogram(texts)\n",
    "   \n",
    "    def save_histogram(self, texts):\n",
    "\n",
    "        \n",
    "        texts = self.add_question_marks(texts)\n",
    "\n",
    "        print(f\"Saving histogram of the REAL text data.\")\n",
    "        char_counts = Counter(\"\".join(texts))  # Example output: [('a', 2), ('d', 1)]\n",
    "        char_counts = dict(char_counts)\n",
    "        print(f\"char_counts: {char_counts}\")\n",
    "        c = [char_counts[v] for v in self.vocab if v not in [\"p\"]]  # Exclude padding and silence tokens\n",
    "        c = np.array(c, dtype=np.float32)\n",
    "        c /= c.sum()  # Normalize the counts to get probabilities\n",
    "        \n",
    "        self.prior = c # save the counts as prior for kl loss.\n",
    "        \n",
    "        # Plotting the histogram\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(self.vocab[1:], c, color='blue', alpha=0.7)\n",
    "        plt.xlabel('Codebook Entry (Char)')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.title('Codebook Usage Distribution')\n",
    "        plt.grid(axis='y')\n",
    "        plt.savefig('REAL_codebook_usage_distribution.png', bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "    def add_question_marks(self, texts=[]):\n",
    "        print(f\"Preprocessing the text data by adding silence tokens.\")\n",
    "        \n",
    "        modified_texts = []\n",
    "        for sentence in tqdm(texts):\n",
    "            modified_sentence = ['?']# Add question marks at start \n",
    "            previous_char = None\n",
    "            for char in sentence:\n",
    "                # if  char == previous_char insert a question mark\n",
    "                if previous_char == char:\n",
    "                    modified_sentence.append(\"?\")\n",
    "                \n",
    "                modified_sentence.append(char)\n",
    "    \n",
    "                # Randomly insert question marks with 0.25 probability\n",
    "                if random.random() < 0.25 and modified_sentence[-1] != '?':\n",
    "                    modified_sentence.append(\"?\")\n",
    "                \n",
    "                previous_char = char\n",
    "                    \n",
    "            if modified_sentence[-1] != '?': \n",
    "                modified_sentence.append(\"?\")  # Add a question mark at the end\n",
    "            modified_texts.append(\"\".join(modified_sentence))\n",
    "        print(f\"Preprocessing done.\")\n",
    "        print(f\"Modified text sample\")\n",
    "        print(f\"{random.choice(modified_texts)}\")\n",
    "        print(f\"{random.choice(modified_texts)}\")\n",
    "        \n",
    "        return  modified_texts\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        Creates a sorted list of unique characters with special tokens.\n",
    "        special_tokens = [\"p\", \"?\"]  # \"p\" = PAD, \"?\" = silence\n",
    "        \"\"\"\n",
    "        unique_chars = sorted(set(\"\".join(texts)))\n",
    "        return [\"p\"] + unique_chars + [\"?\"]\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encodes text into a list of indices.\"\"\"\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "\n",
    "    def decode(self, indices, keep_special_tokens=False):\n",
    "        \"\"\"Decodes indices back into text, removing all special tokens.\"\"\"\n",
    "        if keep_special_tokens:\n",
    "            return \"\".join(self.idx_to_char[idx] for idx in indices)\n",
    "        return \"\".join(self.idx_to_char[idx] for idx in indices if self.idx_to_char[idx] not in {\"p\", \"?\"})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        result = [\"?\"]\n",
    "        prev_char = \"\"\n",
    "\n",
    "        for char in text:\n",
    "            if char == prev_char:\n",
    "                result.append(\"?\")\n",
    "            result.append(char)\n",
    "\n",
    "            # Slightly more efficient: only check random if not already '?'\n",
    "            if result[-1] != \"?\" and random.random() < 0.25:\n",
    "                result.append(\"?\")\n",
    "\n",
    "            prev_char = char\n",
    "\n",
    "        # Ensure it ends with a question mark\n",
    "        if result[-1] != \"?\":\n",
    "            result.append(\"?\")\n",
    "\n",
    "        modified_text = \"\".join(result)\n",
    "        input_ids = self.encode(modified_text)\n",
    "        return input_ids\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        inp = [item for item in batch]\n",
    "        pad_token_id = self.char_to_idx['p']\n",
    "        max_length = max(len(seq) for seq in inp)\n",
    "\n",
    "        # Pad sequences\n",
    "        def pad_sequence(seq, max_length):\n",
    "            return seq + [pad_token_id] * (max_length - len(seq))\n",
    "\n",
    "        inp = torch.tensor([pad_sequence(seq, max_length) for seq in inp], dtype=torch.long)\n",
    "        mask = torch.tensor([[False] * len(seq) + [True] * (max_length - len(seq)) for seq in batch], dtype=torch.bool)\n",
    "    \n",
    "        return inp, mask.unsqueeze(-1)\n",
    "        \n",
    "dataset = Dataset_txt(data=\"/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train_norm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
