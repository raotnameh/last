{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK', \"THAT HAD ITS SOURCE AWAY BACK IN THE WOODS OF THE OLD CUTHBERT PLACE IT WAS REPUTED TO BE AN INTRICATE HEADLONG BROOK IN ITS EARLIER COURSE THROUGH THOSE WOODS WITH DARK SECRETS OF POOL AND CASCADE BUT BY THE TIME IT REACHED LYNDE'S HOLLOW IT WAS A QUIET WELL CONDUCTED LITTLE STREAM\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281241/281241 [25:56<00:00, 180.69it/s] \n"
     ]
    }
   ],
   "source": [
    "from g2p_en import G2p\n",
    "import os\n",
    "\n",
    "from transformers import RobertaTokenizer, BertTokenizer, GPT2Tokenizer, LlamaTokenizerFast, AutoTokenizer\n",
    "\n",
    "# Load the RoBERTa tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "# tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "# Initialize G2P model\n",
    "g2p = G2p()\n",
    "\n",
    "# Function to convert transcript to phonemes\n",
    "def convert_to_phonemes(text):\n",
    "    return g2p(text)\n",
    "\n",
    "# reading dataset. \n",
    "with open(\"../data/transcription.txt\", \"r\") as f:\n",
    "    out = f.readlines()\n",
    "\n",
    "out = [x.split(\"\\t\")[1].strip() for x in out]\n",
    "print(out[:2])\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "w, p, s, c = 0, 0, 0, 0\n",
    "for text in tqdm(out):\n",
    "    phonemes = convert_to_phonemes(text)  # Convert to phonemes\n",
    "    phonemes = [p for p in phonemes if p != ' ']  # Remove empty phonemes\n",
    "    charcters = [c for c in text if c != ' ']  # Remove empty characters\n",
    "    \n",
    "    subwords = tokenizer.tokenize(text) # Convert the word to subwords\n",
    "    \n",
    "    w += len(text.split(\" \"))\n",
    "    c += len(charcters)\n",
    "    p += len(phonemes)\n",
    "    s += len(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 9403555, Phonemes: 33540294, Subwords: 10295687, Characters: 40659898\n",
      "character to word ratio: 4.323885807016602\n",
      "Phoneme to word ratio: 3.566767461880108\n",
      "Subword to word ratio: 1.094871779874739\n"
     ]
    }
   ],
   "source": [
    "print(f\"Words: {w}, Phonemes: {p}, Subwords: {s}, Characters: {c}\")\n",
    "print(f\"character to word ratio: {c/w}\")\n",
    "print(f\"Phoneme to word ratio: {p/w}\")\n",
    "print(f\"Subword to word ratio: {s/w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of vocabulary in the tokenizer\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size of the g2p model\n",
    "g2p = G2p()\n",
    "len(g2p.phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaTokenizerFast\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "\n",
    "# # Get full vocabulary (token -> index mapping)\n",
    "# vocab = tokenizer.get_vocab()\n",
    "\n",
    "# # Extract indices of English-only tokens\n",
    "# english_indices = [\n",
    "#     v for k, v in vocab.items() if (k.replace(\"▁\", \"\").isalpha() and k.isascii())\n",
    "# ]\n",
    "\n",
    "# print(\"English token indices:\", english_indices[:20])  # Show first 20 indices\n",
    "# print(\"Total English tokens:\", len(english_indices))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
