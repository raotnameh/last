checkpoint: 
  step: 10000

logging: 
  dir: "/raid/home/rajivratn/hemant_rajivratn/grpo/src/logs"
  step: 10

device: "cuda"

dataset_speech:
  train_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/train.tsv"
  val_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/val.tsv"
  test_path: "/raid/home/rajivratn/hemant_rajivratn/last/data/librispeech/manifest/test.tsv"

  min_duration: 32000
  max_duration: 160000
  batch_size: &batch_size_value 16

dataset_txt:
  path: "/raid/home/rajivratn/hemant_rajivratn/last/data/txt/train.wrd"
  batch_size: *batch_size_value
  skip_non_speech: True

train:
  seed: 42123

  temp: 1.0
  groups: 128 # this is the beam size.

  lr: 0.00005
  grad_clip: 1.0
  
  steps: 100000
  freeze_epochs: 1
  accumulation_steps: 1

  resume_checkpoint: False
  resume_path: "checkpoints/step_10000.pth"

lr_scheduler:
  phase_ratio: [0.01, 0.0, 0.99]

codebook:
  # model_name: "meta-llama/Llama-3.2-1B"
  model_name: "gpt2"

encoder:
  ckpt_path: "/raid/home/rajivratn/hemant_rajivratn/last/weights/hubert_base_ls960.pt"
  # ckpt_path: "/raid/home/rajivratn/hemant_rajivratn/last/weights/convert_iter3.pt"
  encoder_embed_dim: 768
  frozen_layers: ["11"]

downsample:
  kernel_size: 5
  stride: 1
  groups: 128 # do not touch it is downsample
